{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning TinyLlama-1.1B-Chat for Mathematical Reasoning"
      ],
      "metadata": {
        "id": "IbtLKlU4R4-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*Environment setup*"
      ],
      "metadata": {
        "id": "DEiPbVZ3SbQM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKWupoaz6Z0x"
      },
      "outputs": [],
      "source": [
        "!pip install -q h5py typing-extensions wheel\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display GPU information\n",
        "!nvidia-smi\n",
        "\n",
        "# Create output directory\n",
        "!mkdir -p model_outputs"
      ],
      "metadata": {
        "id": "zKjv77Wx6fRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*Imports and Configuration*"
      ],
      "metadata": {
        "id": "NosjHXw3Sefs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset, Dataset, concatenate_datasets\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "import re\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "N7O7vqd76fXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # 1.1B parameter model\n",
        "print(f\"Using model: {MODEL_NAME}\")\n",
        "SAVE_DIRECTORY = \"tinyllama_math\"\n",
        "os.makedirs(SAVE_DIRECTORY, exist_ok=True)"
      ],
      "metadata": {
        "id": "54iZcZcb6fcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()"
      ],
      "metadata": {
        "id": "GDJtGPyl6ffA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Loading and Preparation"
      ],
      "metadata": {
        "id": "Ndg5sJjWSiTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_combined_math_datasets(max_examples=10000):\n",
        "    \"\"\"Load and combine multiple math datasets for more robust training\"\"\"\n",
        "    print(f\"Loading multiple math datasets (up to {max_examples} examples total)...\")\n",
        "    datasets = []\n",
        "\n",
        "    # 1. Load NuminaMath dataset (primary source)\n",
        "    try:\n",
        "        numina_dataset = load_dataset(\"PrimeIntellect/NuminaMath-QwQ-CoT-5M\")\n",
        "        # Sample more data - at least 5000 examples if available\n",
        "        sampled_numina = numina_dataset[\"train\"].shuffle(seed=42).select(\n",
        "            range(min(5000, len(numina_dataset[\"train\"])))\n",
        "        )\n",
        "        datasets.append(sampled_numina)\n",
        "        print(f\"Added {len(sampled_numina)} examples from NuminaMath\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading NuminaMath: {e}\")\n",
        "\n",
        "    # 2. Add GSM8K training data\n",
        "    try:\n",
        "        gsm8k_train = load_dataset(\"gsm8k\", \"main\")[\"train\"]\n",
        "        datasets.append(gsm8k_train)\n",
        "        print(f\"Added {len(gsm8k_train)} examples from GSM8K train set\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading GSM8K: {e}\")\n",
        "\n",
        "    # 3. Augment with examples specifically oriented toward step-by-step reasoning\n",
        "    try:\n",
        "        cot_dataset = load_dataset(\"reasoning-machines/gsm-hard\", split=\"train\")\n",
        "        sampled_cot = cot_dataset.shuffle(seed=42).select(range(min(1000, len(cot_dataset))))\n",
        "        datasets.append(sampled_cot)\n",
        "        print(f\"Added {len(sampled_cot)} examples from reasoning-machines/gsm-hard\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading reasoning-machines dataset: {e}\")\n",
        "\n",
        "    # Combine all datasets\n",
        "    if not datasets:\n",
        "        raise ValueError(\"Failed to load any datasets\")\n",
        "\n",
        "    # Shuffle and limit total size\n",
        "    combined_data = concatenate_datasets(datasets)\n",
        "    combined_data = combined_data.shuffle(seed=42)\n",
        "    if len(combined_data) > max_examples:\n",
        "        combined_data = combined_data.select(range(max_examples))\n",
        "\n",
        "    print(f\"Final combined dataset size: {len(combined_data)} examples\")\n",
        "    return combined_data"
      ],
      "metadata": {
        "id": "muMb8JBB6fhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_gsm8k_evaluation():\n",
        "    \"\"\"Load GSM8k test set for evaluation\"\"\"\n",
        "    print(\"Loading GSM8k for evaluation...\")\n",
        "    gsm8k_dataset = load_dataset(\"gsm8k\", \"main\")\n",
        "    test_set = gsm8k_dataset[\"test\"]\n",
        "    print(f\"GSM8k test set loaded. Size: {len(test_set)}\")\n",
        "    return test_set"
      ],
      "metadata": {
        "id": "LfKOhfQD6fj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*Create Evaluation Dataset*"
      ],
      "metadata": {
        "id": "m8WaJ9fJSui0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_evaluation_set(gsm8k_eval, size=200):\n",
        "    \"\"\"Create a larger evaluation set for more reliable metrics\"\"\"\n",
        "    print(\"Creating evaluation dataset...\")\n",
        "    gsm8k_samples = gsm8k_eval.select(range(min(size, len(gsm8k_eval))))\n",
        "\n",
        "    eval_set = []\n",
        "    for sample in gsm8k_samples:\n",
        "        solution = sample[\"answer\"]\n",
        "        # Extract the answer value from the solution more carefully\n",
        "        answer_matches = re.findall(r\"####\\s*([-+]?\\d*\\.?\\d+)\", solution)\n",
        "        if answer_matches:\n",
        "            answer_value = answer_matches[-1].strip()\n",
        "        else:\n",
        "            # Fallback to the whole section after ####\n",
        "            parts = solution.split(\"####\")\n",
        "            if len(parts) > 1:\n",
        "                answer_value = parts[-1].strip()\n",
        "            else:\n",
        "                # Last resort: take the last number in the solution\n",
        "                numbers = re.findall(r\"([-+]?\\d*\\.?\\d+)\", solution)\n",
        "                answer_value = numbers[-1] if numbers else solution.strip()\n",
        "\n",
        "        eval_set.append({\n",
        "            \"category\": \"math_reasoning\",\n",
        "            \"question\": sample[\"question\"],\n",
        "            \"solution\": solution,\n",
        "            \"answer\": answer_value\n",
        "        })\n",
        "\n",
        "    # Save the evaluation set\n",
        "    with open(os.path.join(SAVE_DIRECTORY, \"evaluation_set.json\"), \"w\") as f:\n",
        "        json.dump(eval_set, f)\n",
        "\n",
        "    print(f\"Created evaluation set with {len(eval_set)} samples\")\n",
        "    return eval_set"
      ],
      "metadata": {
        "id": "sd5Y_oVi7Goe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*Model Evaluation Functions*"
      ],
      "metadata": {
        "id": "FJCJzKClS01P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_and_tokenizer(model_name, load_in_4bit=True):\n",
        "    \"\"\"Load model and tokenizer with optimized settings\"\"\"\n",
        "    print(f\"Loading {model_name}...\")\n",
        "\n",
        "    # Configure quantization with optimal settings\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=load_in_4bit,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        "    )\n",
        "\n",
        "    # Load tokenizer with special token handling\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Ensure we have proper padding token\n",
        "    if not tokenizer.pad_token:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    print(\"Loading model with memory-efficient settings...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=quantization_config,\n",
        "        torch_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "xwWNszd37Gsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*Format the prompt for TinyLlama Chat models*"
      ],
      "metadata": {
        "id": "kUsc8IQhTJ8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_prompt_for_tinyllama(question):\n",
        "    \"\"\"Format prompt with more explicit instructions for mathematical reasoning\"\"\"\n",
        "    return f\"\"\"<|system|>\n",
        "You are a highly intelligent math assistant that excels at solving complex math problems step by step using chain-of-thought reasoning. You always show your work clearly, explaining each step of your calculation, and double-check your final answer.\n",
        "<|user|>\n",
        "Please solve this math problem by breaking it down into clearly defined steps. Show all your work and calculations, and make sure to verify your answer.\n",
        "\n",
        "Problem: {question}\n",
        "\n",
        "First understand the problem, identify what is being asked, plan your solution approach, and then solve it step-by-step. After reaching your final answer, include it at the end after '####'.\n",
        "<|assistant|>\"\"\"\n"
      ],
      "metadata": {
        "id": "66Zz4j547GvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*Extract the final answer from model responses*"
      ],
      "metadata": {
        "id": "0aOfW06lS_s2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_answer(response, question_type=\"math_reasoning\"):\n",
        "    \"\"\"Extract the final answer with improved regex patterns\"\"\"\n",
        "    if question_type == \"math_reasoning\":\n",
        "        # First try to find answers after #### marker with number extraction\n",
        "        hash_matches = re.search(r\"#{3,}\\s*([-+]?\\d*\\.?\\d+)\", response, re.DOTALL)\n",
        "        if hash_matches:\n",
        "            return hash_matches.group(1).strip()\n",
        "\n",
        "        # Try to find \"the answer is X\" pattern\n",
        "        answer_is_pattern = re.search(r\"(?:the\\s+answer\\s+is|final\\s+answer\\s*[:=])\\s*([-+]?\\d*\\.?\\d+)\",\n",
        "                                     response.lower(), re.DOTALL)\n",
        "        if answer_is_pattern:\n",
        "            return answer_is_pattern.group(1).strip()\n",
        "\n",
        "        # Try to find \"Therefore, X\" pattern\n",
        "        therefore_pattern = re.search(r\"therefore,?\\s*([-+]?\\d*\\.?\\d+)\",\n",
        "                                     response.lower(), re.DOTALL)\n",
        "        if therefore_pattern:\n",
        "            return therefore_pattern.group(1).strip()\n",
        "\n",
        "        # Look for the last number after a logical conclusion marker\n",
        "        conclusion_pattern = re.search(r\"(?:so|thus|hence|finally|in\\s+conclusion),?\\s*([-+]?\\d*\\.?\\d+)\",\n",
        "                                      response.lower(), re.DOTALL)\n",
        "        if conclusion_pattern:\n",
        "            return conclusion_pattern.group(1).strip()\n",
        "\n",
        "        # Look for the last number in the response\n",
        "        numbers = re.findall(r\"([-+]?\\d*\\.?\\d+)\", response)\n",
        "        if numbers:\n",
        "            return numbers[-1].strip()\n",
        "\n",
        "        return response.strip()\n",
        "    else:\n",
        "        # For other question types, just return the last sentence or phrase\n",
        "        response = response.strip()\n",
        "        sentences = response.split(\".\")\n",
        "        if sentences:\n",
        "            last_sentence = sentences[-1].strip()\n",
        "            if len(last_sentence) > 50:\n",
        "                last_sentence = last_sentence[-50:].strip()\n",
        "            return last_sentence\n",
        "\n",
        "        # Fallback\n",
        "        if len(response) > 50:\n",
        "            return response[-50:].strip()\n",
        "        return response"
      ],
      "metadata": {
        "id": "Stjvgswg7Gxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*Evaluate Model on Evaluation Set*"
      ],
      "metadata": {
        "id": "D2xUMwxSTXoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, tokenizer, evaluation_data, model_name, show_examples=True):\n",
        "    \"\"\"Evaluate model with more robust answer extraction and error handling\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for idx, item in enumerate(tqdm(evaluation_data, desc=f\"Evaluating {model_name}\")):\n",
        "        try:\n",
        "            question = item[\"question\"]\n",
        "            true_answer = item[\"answer\"]\n",
        "            category = item[\"category\"]\n",
        "\n",
        "            # Prepare prompt\n",
        "            prompt = format_prompt_for_tinyllama(question)\n",
        "\n",
        "            # Tokenize with proper handling\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
        "            input_ids = inputs.input_ids.to(model.device)\n",
        "\n",
        "            # Generate with improved settings for math reasoning\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    input_ids,\n",
        "                    max_new_tokens=512,  # Longer output for complete reasoning\n",
        "                    temperature=0.1,     # Lower temperature for more deterministic answers\n",
        "                    top_p=0.92,          # Slightly narrower sampling\n",
        "                    do_sample=True,      # Still use sampling for some diversity\n",
        "                    num_beams=2,         # Simple beam search for better quality\n",
        "                    pad_token_id=tokenizer.pad_token_id,\n",
        "                    repetition_penalty=1.1  # Slight penalty to avoid loops\n",
        "                )\n",
        "\n",
        "            # Decode and extract response\n",
        "            full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            # For TinyLlama, extract after <|assistant|>\n",
        "            assistant_part = full_output.split(\"<|assistant|>\")\n",
        "            if len(assistant_part) > 1:\n",
        "                response = assistant_part[1].strip()\n",
        "            else:\n",
        "                response = full_output[len(prompt):].strip()\n",
        "\n",
        "            # Extract the answer using improved extraction\n",
        "            extracted_answer = extract_answer(response, category)\n",
        "\n",
        "            # Check if correct with flexible numeric matching\n",
        "            is_correct = False\n",
        "            if category == \"math_reasoning\":\n",
        "                extracted_clean = extracted_answer.replace(\",\", \"\").replace(\"$\", \"\")\n",
        "                true_clean = true_answer.replace(\",\", \"\").replace(\"$\", \"\")\n",
        "\n",
        "                # Clean up numbers for comparison\n",
        "                extracted_numeric = re.sub(r'[^\\d.-]', '', extracted_clean)\n",
        "                true_numeric = re.sub(r'[^\\d.-]', '', true_clean)\n",
        "\n",
        "                try:\n",
        "                    # Try to compare as numbers (allowing for small floating point differences)\n",
        "                    extracted_float = float(extracted_numeric)\n",
        "                    true_float = float(true_numeric)\n",
        "                    is_correct = abs(extracted_float - true_float) < 1e-6\n",
        "                except:\n",
        "                    # If conversion fails, compare as strings\n",
        "                    is_correct = extracted_numeric == true_numeric\n",
        "\n",
        "            results.append({\n",
        "                \"category\": category,\n",
        "                \"question\": question,\n",
        "                \"true_answer\": true_answer,\n",
        "                \"model_response\": response,\n",
        "                \"extracted_answer\": extracted_answer,\n",
        "                \"is_correct\": is_correct\n",
        "            })\n",
        "\n",
        "            # Display some examples during evaluation\n",
        "            if show_examples and idx < 3:\n",
        "                print(f\"\\nExample {idx+1}:\")\n",
        "                print(f\"Question: {question[:100]}...\")\n",
        "                print(f\"True answer: {true_answer}\")\n",
        "                print(f\"Extracted answer: {extracted_answer}\")\n",
        "                print(f\"Correct: {is_correct}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating example {idx}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Calculate overall accuracy\n",
        "    correct = sum(1 for r in results if r[\"is_correct\"])\n",
        "    accuracy = correct / len(results) if results else 0\n",
        "\n",
        "    print(f\"\\n{model_name} Evaluation Results:\")\n",
        "    print(f\"Overall Accuracy: {accuracy:.4f} ({correct}/{len(results)})\")\n",
        "\n",
        "    return {\n",
        "        \"model_name\": model_name,\n",
        "        \"overall_accuracy\": accuracy,\n",
        "        \"detailed_results\": results\n",
        "    }"
      ],
      "metadata": {
        "id": "kQbo1Z8V7Gz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*Process the dataset for training with memory optimization*"
      ],
      "metadata": {
        "id": "19aPup-cTkc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data_for_training(data, tokenizer, max_length=1536):\n",
        "    \"\"\"Process dataset with improved handling for variable data structures and longer contexts\"\"\"\n",
        "    print(f\"Processing dataset ({len(data)} examples) for training...\")\n",
        "\n",
        "    # Identify correct field names based on dataset structure\n",
        "    sample = data[0]\n",
        "    if 'prompt' in sample and 'response' in sample:\n",
        "        question_key, answer_key = 'prompt', 'response'\n",
        "    elif 'question' in sample and 'answer' in sample:\n",
        "        question_key, answer_key = 'question', 'answer'\n",
        "    elif 'problem' in sample and 'solution' in sample:\n",
        "        question_key, answer_key = 'problem', 'solution'\n",
        "    elif 'input' in sample and 'output' in sample:\n",
        "        question_key, answer_key = 'input', 'output'\n",
        "    else:\n",
        "        # Make best guess at field names for other datasets\n",
        "        fields = list(sample.keys())\n",
        "        question_candidates = [k for k in fields if any(term in k.lower() for term in\n",
        "                                                      ['question', 'prompt', 'problem', 'input', 'query'])]\n",
        "        answer_candidates = [k for k in fields if any(term in k.lower() for term in\n",
        "                                                    ['answer', 'response', 'solution', 'output', 'completion'])]\n",
        "\n",
        "        if question_candidates and answer_candidates:\n",
        "            question_key, answer_key = question_candidates[0], answer_candidates[0]\n",
        "        else:\n",
        "            # Use the first two string fields as a last resort\n",
        "            string_fields = [k for k in fields if isinstance(sample[k], str)]\n",
        "            if len(string_fields) >= 2:\n",
        "                question_key, answer_key = string_fields[0], string_fields[1]\n",
        "            else:\n",
        "                raise ValueError(f\"Cannot identify question and answer fields in dataset with keys: {fields}\")\n",
        "\n",
        "    print(f\"Using fields - Question: '{question_key}', Answer: '{answer_key}'\")\n",
        "\n",
        "    def process_batch(batch):\n",
        "        formatted_texts = []\n",
        "        attention_masks = []\n",
        "\n",
        "        for i in range(len(batch[question_key])):\n",
        "            try:\n",
        "                question = batch[question_key][i]\n",
        "                answer = batch[answer_key][i]\n",
        "\n",
        "                # Skip invalid entries\n",
        "                if not isinstance(question, str) or not isinstance(answer, str):\n",
        "                    continue\n",
        "\n",
        "                # Ensure answers end with the #### marker if not already present\n",
        "                if '####' not in answer:\n",
        "                    # Try to find the final numerical answer\n",
        "                    numbers = re.findall(r\"([-+]?\\d*\\.?\\d+)\", answer)\n",
        "                    if numbers:\n",
        "                        final_number = numbers[-1].strip()\n",
        "                        # Only add #### if we're not already at the end of the answer\n",
        "                        if not answer.strip().endswith(final_number):\n",
        "                            answer = answer.strip() + f\"\\n\\n#### {final_number}\"\n",
        "\n",
        "                # Format for TinyLlama with improved prompt\n",
        "                formatted_text = f\"\"\"<|system|>\n",
        "You are a highly intelligent math assistant that excels at solving complex math problems step by step using chain-of-thought reasoning. You always show your work clearly, explaining each step of your calculation, and double-check your final answer.\n",
        "<|user|>\n",
        "Please solve this math problem by breaking it down into clearly defined steps. Show all your work and calculations, and make sure to verify your answer.\n",
        "\n",
        "Problem: {question}\n",
        "\n",
        "First understand the problem, identify what is being asked, plan your solution approach, and then solve it step-by-step. After reaching your final answer, include it at the end after '####'.\n",
        "<|assistant|>\n",
        "{answer}\"\"\"\n",
        "\n",
        "                # Tokenize with dynamic length handling\n",
        "                tokenized = tokenizer(\n",
        "                    formatted_text,\n",
        "                    truncation=True,\n",
        "                    max_length=max_length,\n",
        "                    padding=\"max_length\",\n",
        "                    return_attention_mask=True\n",
        "                )\n",
        "\n",
        "                formatted_texts.append(tokenized[\"input_ids\"])\n",
        "                attention_masks.append(tokenized[\"attention_mask\"])\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing item: {e}\")\n",
        "                continue\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": formatted_texts,\n",
        "            \"attention_mask\": attention_masks\n",
        "        }\n",
        "\n",
        "    # Process in batches for memory efficiency\n",
        "    batch_size = 100\n",
        "    processed_dataset = Dataset.from_dict({\"input_ids\": [], \"attention_mask\": []})\n",
        "\n",
        "    for i in range(0, len(data), batch_size):\n",
        "        batch_end = min(i + batch_size, len(data))\n",
        "        batch = {k: data[k][i:batch_end] for k in data.column_names}\n",
        "        batch_processed = process_batch(batch)\n",
        "\n",
        "        if batch_processed[\"input_ids\"]:\n",
        "            batch_dataset = Dataset.from_dict(batch_processed)\n",
        "            processed_dataset = concatenate_datasets([processed_dataset, batch_dataset])\n",
        "\n",
        "        if i % 500 == 0:\n",
        "            print(f\"Processed {i}/{len(data)} examples...\")\n",
        "            # Free memory\n",
        "            clear_memory()\n",
        "\n",
        "    print(f\"Final processed dataset size: {len(processed_dataset)} examples\")\n",
        "    return processed_dataset"
      ],
      "metadata": {
        "id": "viHzpFUV7G4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*Create Labels for Training*"
      ],
      "metadata": {
        "id": "ePiAjVg5TvQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_labels_from_input_ids(batched_input_ids, tokenizer):\n",
        "    labels = []\n",
        "\n",
        "    for input_ids in batched_input_ids:\n",
        "        # Convert input_ids to string\n",
        "        full_text = tokenizer.decode(input_ids)\n",
        "\n",
        "        # For TinyLlama, find assistant's part\n",
        "        parts = full_text.split(\"<|assistant|>\")\n",
        "        if len(parts) > 1:\n",
        "            user_text = parts[0] + \"<|assistant|>\"\n",
        "        else:\n",
        "            user_text = full_text\n",
        "\n",
        "        # Tokenize user part\n",
        "        user_ids = tokenizer(user_text, add_special_tokens=False)[\"input_ids\"]\n",
        "\n",
        "        # Create labels with -100 for user part\n",
        "        label = [-100] * min(len(user_ids), len(input_ids))\n",
        "\n",
        "        # Fill the rest with actual values\n",
        "        if len(user_ids) < len(input_ids):\n",
        "            label.extend(input_ids[len(user_ids):])\n",
        "\n",
        "        labels.append(label)\n",
        "\n",
        "    return labels"
      ],
      "metadata": {
        "id": "Vrt1Giqk7G6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*Fine-tune Model using LoRA*"
      ],
      "metadata": {
        "id": "YGe6WoCyT4EU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune_model(model_name, training_data, tokenizer=None, output_dir=None):\n",
        "    \"\"\"Fine-tune model with improved LoRA configuration and training strategy\"\"\"\n",
        "    if output_dir is None:\n",
        "        output_dir = os.path.join(SAVE_DIRECTORY, f\"ft_{model_name.split('/')[-1].lower().replace('-', '_')}\")\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Starting improved fine-tuning for {model_name}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # Configure quantization for memory efficiency\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    # Load model and tokenizer if not provided\n",
        "    if tokenizer is None:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        if not tokenizer.pad_token:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    print(\"Loading model with memory efficiency settings...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    # Prepare model for training with advanced settings\n",
        "    model.gradient_checkpointing_enable()\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    # Add labels to dataset\n",
        "    def add_labels(examples):\n",
        "        \"\"\"Add causal language modeling labels with proper masking\"\"\"\n",
        "        labels = []\n",
        "        for input_ids in examples[\"input_ids\"]:\n",
        "            # Convert to tensor if it's not already\n",
        "            if not isinstance(input_ids, torch.Tensor):\n",
        "                input_ids = torch.tensor(input_ids)\n",
        "\n",
        "            # Convert to string to find assistant part\n",
        "            full_text = tokenizer.decode(input_ids)\n",
        "            parts = full_text.split(\"<|assistant|>\")\n",
        "\n",
        "            if len(parts) > 1:\n",
        "                # Get the position of <|assistant|> token\n",
        "                assistant_start = len(tokenizer.encode(parts[0] + \"<|assistant|>\", add_special_tokens=False))\n",
        "\n",
        "                # Create labels with -100 for user part and actual values for assistant part\n",
        "                label = [-100] * min(assistant_start, len(input_ids))\n",
        "\n",
        "                # Fill the rest with actual values\n",
        "                if assistant_start < len(input_ids):\n",
        "                    label.extend(input_ids[assistant_start:].tolist())\n",
        "            else:\n",
        "                # If no assistant part is found, use full input_ids as labels (fallback)\n",
        "                label = input_ids.tolist()\n",
        "\n",
        "            labels.append(label)\n",
        "\n",
        "        examples[\"labels\"] = labels\n",
        "        return examples\n",
        "\n",
        "    print(\"Preparing dataset with labels...\")\n",
        "    if not isinstance(training_data, Dataset):\n",
        "        training_data = process_data_for_training(training_data, tokenizer)\n",
        "\n",
        "    training_data = training_data.map(add_labels, batched=True, batch_size=100)\n",
        "\n",
        "    # Configure IMPROVED LoRA for memory-efficient fine-tuning\n",
        "    # Higher rank (r) and alpha for more expressive adaptations\n",
        "    peft_config = LoraConfig(\n",
        "        r=32,                # Increased from 16 to 32 for more capacity\n",
        "        lora_alpha=64,       # Increased from 32 to 64 for stronger updates\n",
        "        lora_dropout=0.1,    # Increased dropout for better generalization\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\n",
        "            \"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",    # Attention modules\n",
        "            \"gate_proj\", \"up_proj\", \"down_proj\",       # MLP modules\n",
        "            \"W_pack\"                                   # Special module for packed weights\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    print(f\"Model prepared with improved LoRA adapters (rank={peft_config.r}, alpha={peft_config.lora_alpha})\")\n",
        "\n",
        "    # Define improved training arguments with longer training\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=3,                # Increased from 1 to 3 epochs\n",
        "        per_device_train_batch_size=8,     # Increased if memory allows\n",
        "        gradient_accumulation_steps=4,     # Increased for larger effective batch size\n",
        "        learning_rate=1e-4,                # Slightly lower learning rate for stability\n",
        "        weight_decay=0.05,                 # Increased weight decay for regularization\n",
        "        warmup_ratio=0.05,                 # Slightly increased warmup\n",
        "        max_grad_norm=0.5,                 # Increased for stability\n",
        "        logging_steps=10,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=100,\n",
        "        save_total_limit=3,\n",
        "        optim=\"paged_adamw_32bit\",         # Memory-efficient optimizer\n",
        "        fp16=True,                         # Mixed precision\n",
        "        gradient_checkpointing=True,       # Memory efficiency\n",
        "        report_to=\"none\",                  # Disable wandb/tensorboard to save memory\n",
        "        seed=42,\n",
        "        lr_scheduler_type=\"cosine\",        # Better scheduler for math tasks\n",
        "    )\n",
        "\n",
        "    # Create trainer with improved data collator\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False\n",
        "    )\n",
        "\n",
        "    # Update the Trainer with our improved configuration\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=training_data,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    print(f\"Starting training for {training_args.num_train_epochs} epochs...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the final model\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    print(f\"Model saved to {output_dir}\")\n",
        "\n",
        "    # Clear memory\n",
        "    del model\n",
        "    del trainer\n",
        "    clear_memory()\n",
        "\n",
        "    return output_dir"
      ],
      "metadata": {
        "id": "9cCAgEfn7nm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*Evaluate Fine-tuned Model and Compare Results*"
      ],
      "metadata": {
        "id": "w3VGei0YUFzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main execution flow with improvements at every stage\"\"\"\n",
        "    # 1. Load and prepare larger, more diverse datasets\n",
        "    combined_data = load_combined_math_datasets(max_examples=10000)  # 10K examples\n",
        "\n",
        "    # 2. Create a larger evaluation set\n",
        "    gsm8k_eval = load_gsm8k_evaluation()\n",
        "    evaluation_set = create_evaluation_set(gsm8k_eval, size=200)  # 200 eval examples\n",
        "\n",
        "    # 3. Load and evaluate base model first\n",
        "    print(\"\\nEvaluating base TinyLlama model...\")\n",
        "    base_model, base_tokenizer = load_model_and_tokenizer(MODEL_NAME)\n",
        "    base_results = evaluate_model(base_model, base_tokenizer, evaluation_set, \"Base TinyLlama\")\n",
        "\n",
        "    # Save base results\n",
        "    with open(os.path.join(SAVE_DIRECTORY, \"base_model_results.json\"), \"w\") as f:\n",
        "        json.dump(base_results, f)\n",
        "\n",
        "    # Free memory\n",
        "    del base_model\n",
        "    clear_memory()\n",
        "\n",
        "    # 4. Process data for fine-tuning with improved pipeline\n",
        "    print(\"\\nProcessing dataset for TinyLlama fine-tuning...\")\n",
        "    processed_data = process_data_for_training(combined_data, base_tokenizer, max_length=1536)\n",
        "\n",
        "    # 5. Fine-tune with improved LoRA settings\n",
        "    output_dir = fine_tune_model(\n",
        "        MODEL_NAME,\n",
        "        processed_data,\n",
        "        tokenizer=base_tokenizer,\n",
        "        output_dir=os.path.join(SAVE_DIRECTORY, \"improved_tinyllama_math\")\n",
        "    )\n",
        "\n",
        "    # 6. Evaluate fine-tuned model with robust metrics\n",
        "    print(\"\\nLoading and evaluating fine-tuned model...\")\n",
        "    # First reload the base model\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        device_map=\"auto\",\n",
        "        load_in_4bit=True\n",
        "    )\n",
        "\n",
        "    # Then load our fine-tuned adapter\n",
        "    ft_model = PeftModel.from_pretrained(base_model, output_dir)\n",
        "\n",
        "    # Evaluate\n",
        "    ft_results = evaluate_model(ft_model, base_tokenizer, evaluation_set, \"Fine-tuned TinyLlama\")\n",
        "\n",
        "    # Save results\n",
        "    with open(os.path.join(SAVE_DIRECTORY, \"ft_model_results.json\"), \"w\") as f:\n",
        "        json.dump(ft_results, f)\n",
        "\n",
        "    # 7. Create comparison visualizations\n",
        "    improvement = ft_results[\"overall_accuracy\"] - base_results[\"overall_accuracy\"]\n",
        "    percent_improvement = (improvement / base_results[\"overall_accuracy\"] * 100\n",
        "                         if base_results[\"overall_accuracy\"] > 0 else float('inf'))\n",
        "\n",
        "    print(\"\\n====== MODEL COMPARISON ======\")\n",
        "    print(f\"Base model accuracy: {base_results['overall_accuracy']:.4f}\")\n",
        "    print(f\"Fine-tuned model accuracy: {ft_results['overall_accuracy']:.4f}\")\n",
        "    print(f\"Absolute improvement: {improvement:.4f}\")\n",
        "    print(f\"Relative improvement: {percent_improvement:.2f}%\")\n",
        "\n",
        "    # Create and save comparison chart\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    labels = [\"Base TinyLlama\", \"Fine-tuned TinyLlama\"]\n",
        "    accuracies = [base_results[\"overall_accuracy\"], ft_results[\"overall_accuracy\"]]\n",
        "    bars = plt.bar(labels, accuracies, color=['blue', 'green'])\n",
        "\n",
        "    # Add values on bars\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                f'{height:.4f}', ha='center', va='bottom')\n",
        "\n",
        "    plt.title('Model Accuracy Comparison on GSM8K Math Reasoning')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim(0, max(max(accuracies) + 0.1, 0.5))  # Set reasonable y-limit\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.savefig(os.path.join(SAVE_DIRECTORY, 'model_comparison.png'))\n",
        "\n",
        "    # 8. Show examples of model improvements\n",
        "    print(\"\\n====== EXAMPLE COMPARISONS ======\")\n",
        "    base_examples = base_results[\"detailed_results\"]\n",
        "    ft_examples = ft_results[\"detailed_results\"]\n",
        "\n",
        "    # Find examples where the fine-tuned model is correct but base is wrong\n",
        "    improved_examples = []\n",
        "    for i in range(min(len(base_examples), len(ft_examples))):\n",
        "        if ft_examples[i][\"is_correct\"] and not base_examples[i][\"is_correct\"]:\n",
        "            improved_examples.append((base_examples[i], ft_examples[i]))\n",
        "\n",
        "    print(f\"Found {len(improved_examples)} examples where fine-tuning improved the result\")\n",
        "\n",
        "    # Show a few examples\n",
        "    for i, (base_ex, ft_ex) in enumerate(improved_examples[:3]):\n",
        "        print(f\"\\n--- Example {i+1} where fine-tuning helped ---\")\n",
        "        print(f\"Question: {base_ex['question'][:150]}...\")\n",
        "        print(f\"True answer: {base_ex['true_answer']}\")\n",
        "        print(f\"Base model answer: {base_ex['extracted_answer']} ❌\")\n",
        "        print(f\"Fine-tuned model answer: {ft_ex['extracted_answer']} ✅\")\n",
        "\n",
        "    # Free memory\n",
        "    del base_model\n",
        "    del ft_model\n",
        "    clear_memory()\n",
        "\n",
        "    return base_results, ft_results"
      ],
      "metadata": {
        "id": "GscKijSg7nrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_lora_weights(base_model_name, lora_model_path, merged_model_path, alpha=0.7):\n",
        "    \"\"\"Merge LoRA weights with base model for potentially better performance\n",
        "\n",
        "    Args:\n",
        "        base_model_name: Name of the base model\n",
        "        lora_model_path: Path to the fine-tuned LoRA model\n",
        "        merged_model_path: Path to save the merged model\n",
        "        alpha: Weight to assign to the fine-tuned model (0-1)\n",
        "    \"\"\"\n",
        "    print(f\"\\nMerging LoRA weights into base model with alpha={alpha}...\")\n",
        "\n",
        "    # Load base model in float16 for merging\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    # Load LoRA model\n",
        "    peft_model = PeftModel.from_pretrained(base_model, lora_model_path)\n",
        "\n",
        "    # Merge weights\n",
        "    merged_model = peft_model.merge_and_unload(alpha=alpha)\n",
        "\n",
        "    # Save merged model\n",
        "    merged_model.save_pretrained(merged_model_path)\n",
        "\n",
        "    # Also save tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "    tokenizer.save_pretrained(merged_model_path)\n",
        "\n",
        "    print(f\"Merged model saved to {merged_model_path}\")\n",
        "\n",
        "    # Free memory\n",
        "    del base_model\n",
        "    del peft_model\n",
        "    del merged_model\n",
        "    clear_memory()\n",
        "\n",
        "    return merged_model_path"
      ],
      "metadata": {
        "id": "v-RrCO8z7nt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_merged_model(model_path, evaluation_set, tokenizer_path=None):\n",
        "    \"\"\"Evaluate the merged model on the test set\"\"\"\n",
        "    print(f\"\\nEvaluating merged model at {model_path}...\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    if tokenizer_path is None:\n",
        "        tokenizer_path = model_path\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "    if not tokenizer.pad_token:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Load model\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_path,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.float16\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    results = evaluate_model(model, tokenizer, evaluation_set, \"Merged TinyLlama\")\n",
        "\n",
        "    # Save results\n",
        "    with open(os.path.join(SAVE_DIRECTORY, \"merged_model_results.json\"), \"w\") as f:\n",
        "        json.dump(results, f)\n",
        "\n",
        "    # Free memory\n",
        "    del model\n",
        "    clear_memory()\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "ikvsMuQN7nwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_advanced_fine_tuning_pipeline():\n",
        "    \"\"\"Run the complete fine-tuning pipeline with advanced techniques\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"STARTING ADVANCED TINYLLAMA FINE-TUNING FOR MATHEMATICAL REASONING\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # 1. Create all necessary directories\n",
        "    os.makedirs(SAVE_DIRECTORY, exist_ok=True)\n",
        "    os.makedirs(os.path.join(SAVE_DIRECTORY, \"checkpoints\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(SAVE_DIRECTORY, \"merged_model\"), exist_ok=True)\n",
        "\n",
        "    # 2. Load and prepare datasets with higher quality and quantity\n",
        "    print(\"\\nLoading and preparing datasets...\")\n",
        "    combined_data = load_combined_math_datasets(max_examples=10000)\n",
        "\n",
        "    # 3. Prepare evaluation set\n",
        "    gsm8k_eval = load_gsm8k_evaluation()\n",
        "    evaluation_set = create_evaluation_set(gsm8k_eval, size=200)\n",
        "\n",
        "    # 4. Base model evaluation first\n",
        "    print(\"\\nEvaluating base model...\")\n",
        "    base_model, base_tokenizer = load_model_and_tokenizer(MODEL_NAME)\n",
        "    base_results = evaluate_model(base_model, base_tokenizer, evaluation_set, \"Base TinyLlama\")\n",
        "\n",
        "    # Save base results\n",
        "    with open(os.path.join(SAVE_DIRECTORY, \"base_model_results.json\"), \"w\") as f:\n",
        "        json.dump(base_results, f)\n",
        "\n",
        "    # Free base model memory\n",
        "    del base_model\n",
        "    clear_memory()\n",
        "\n",
        "    # 5. Process data for fine-tuning\n",
        "    print(\"\\nProcessing dataset for fine-tuning...\")\n",
        "    processed_data = process_data_for_training(combined_data, base_tokenizer, max_length=1536)\n",
        "\n",
        "    # 6. Fine-tune with improved LoRA settings\n",
        "    lora_output_dir = fine_tune_model(\n",
        "        MODEL_NAME,\n",
        "        processed_data,\n",
        "        tokenizer=base_tokenizer,\n",
        "        output_dir=os.path.join(SAVE_DIRECTORY, \"checkpoints\", \"improved_tinyllama_math\")\n",
        "    )\n",
        "\n",
        "    # 7. Evaluate fine-tuned LoRA model\n",
        "    print(\"\\nEvaluating fine-tuned LoRA model...\")\n",
        "    # Reload base model\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        device_map=\"auto\",\n",
        "        load_in_4bit=True\n",
        "    )\n",
        "\n",
        "    # Load LoRA adapter\n",
        "    ft_model = PeftModel.from_pretrained(base_model, lora_output_dir)\n",
        "\n",
        "    # Evaluate\n",
        "    ft_results = evaluate_model(ft_model, base_tokenizer, evaluation_set, \"Fine-tuned LoRA TinyLlama\")\n",
        "\n",
        "    # Save results\n",
        "    with open(os.path.join(SAVE_DIRECTORY, \"ft_model_results.json\"), \"w\") as f:\n",
        "        json.dump(ft_results, f)\n",
        "\n",
        "    # Free memory\n",
        "    del base_model\n",
        "    del ft_model\n",
        "    clear_memory()\n",
        "\n",
        "    # 8. Merge weights for potentially better performance\n",
        "    merged_model_path = os.path.join(SAVE_DIRECTORY, \"merged_model\")\n",
        "    merge_lora_weights(\n",
        "        MODEL_NAME,\n",
        "        lora_output_dir,\n",
        "        merged_model_path,\n",
        "        alpha=0.7  # Adjust this weight based on results\n",
        "    )\n",
        "\n",
        "    # 9. Evaluate merged model\n",
        "    merged_results = evaluate_merged_model(\n",
        "        merged_model_path,\n",
        "        evaluation_set,\n",
        "        tokenizer_path=MODEL_NAME\n",
        "    )\n",
        "\n",
        "    # 10. Create comprehensive comparison\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FINAL RESULTS COMPARISON\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Calculate improvements\n",
        "    lora_improvement = ft_results[\"overall_accuracy\"] - base_results[\"overall_accuracy\"]\n",
        "    merged_improvement = merged_results[\"overall_accuracy\"] - base_results[\"overall_accuracy\"]\n",
        "\n",
        "    lora_percent = (lora_improvement / base_results[\"overall_accuracy\"] * 100\n",
        "                  if base_results[\"overall_accuracy\"] > 0 else float('inf'))\n",
        "\n",
        "    merged_percent = (merged_improvement / base_results[\"overall_accuracy\"] * 100\n",
        "                    if base_results[\"overall_accuracy\"] > 0 else float('inf'))\n",
        "\n",
        "    print(f\"Base model accuracy: {base_results['overall_accuracy']:.4f}\")\n",
        "    print(f\"Fine-tuned LoRA model accuracy: {ft_results['overall_accuracy']:.4f} (+{lora_improvement:.4f}, +{lora_percent:.2f}%)\")\n",
        "    print(f\"Merged model accuracy: {merged_results['overall_accuracy']:.4f} (+{merged_improvement:.4f}, +{merged_percent:.2f}%)\")\n",
        "\n",
        "    # Create comparison chart for all three models\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    labels = [\"Base TinyLlama\", \"Fine-tuned LoRA\", \"Merged Model\"]\n",
        "    accuracies = [\n",
        "        base_results[\"overall_accuracy\"],\n",
        "        ft_results[\"overall_accuracy\"],\n",
        "        merged_results[\"overall_accuracy\"]\n",
        "    ]\n",
        "\n",
        "    bars = plt.bar(labels, accuracies, color=['blue', 'green', 'purple'])\n",
        "\n",
        "    # Add values on bars\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                f'{height:.4f}', ha='center', va='bottom')\n",
        "\n",
        "    plt.title('TinyLlama Model Accuracy Comparison on GSM8K Math Reasoning')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim(0, max(max(accuracies) + 0.1, 0.5))\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.savefig(os.path.join(SAVE_DIRECTORY, 'final_model_comparison.png'))\n",
        "\n",
        "    # Return all results\n",
        "    return {\n",
        "        \"base\": base_results,\n",
        "        \"lora\": ft_results,\n",
        "        \"merged\": merged_results\n",
        "    }\n",
        "\n",
        "# Run the main pipeline if executed directly\n",
        "if __name__ == \"__main__\":\n",
        "    results = run_advanced_fine_tuning_pipeline()\n",
        "\n",
        "    # Print final results for report\n",
        "    print(\"\\n====== FINAL RESULTS FOR REPORT ======\")\n",
        "    print(f\"Base Model Accuracy: {results['base']['overall_accuracy']:.4f}\")\n",
        "    print(f\"Fine-tuned LoRA Model Accuracy: {results['lora']['overall_accuracy']:.4f}\")\n",
        "    print(f\"Merged Model Accuracy: {results['merged']['overall_accuracy']:.4f}\")\n",
        "\n",
        "    best_model = max(\n",
        "        [\"Base\", \"LoRA\", \"Merged\"],\n",
        "        key=lambda x: results[x.lower()]['overall_accuracy']\n",
        "    )\n",
        "\n",
        "    print(f\"\\nBest performing model: {best_model}\")\n",
        "    print(\"\\nTraining and evaluation complete!\")"
      ],
      "metadata": {
        "id": "Hrw0nfrACxJZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}