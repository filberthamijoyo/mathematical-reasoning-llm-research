{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Environment"
      ],
      "metadata": {
        "id": "nnKS0D2dBHXM"
      },
      "id": "nnKS0D2dBHXM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f6c6af6",
      "metadata": {
        "id": "6f6c6af6",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install -q h5py typing-extensions wheel\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_T66f30GsT0t",
      "metadata": {
        "id": "_T66f30GsT0t"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p model_outputs"
      ],
      "metadata": {
        "id": "XpmGxNBv6gKk"
      },
      "id": "XpmGxNBv6gKk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a194ac33",
      "metadata": {
        "id": "a194ac33"
      },
      "source": [
        "\n",
        "#Load and Prepare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wye4Wm9rXgFF",
      "metadata": {
        "id": "wye4Wm9rXgFF",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import json\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import gc\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset, Dataset, concatenate_datasets\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer\n",
        "import re\n",
        "from jinja2 import Template\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bpWUSDliX-e2",
      "metadata": {
        "id": "bpWUSDliX-e2"
      },
      "outputs": [],
      "source": [
        "# radom for reproducibility\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Improved memory clearing function\"\"\"\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "    print(\"Memory cleared\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set environment variables to limit memory usage\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
        "\n",
        "# Clear all memory before starting\n",
        "clear_memory()"
      ],
      "metadata": {
        "id": "mN13XxJu4KI2"
      },
      "id": "mN13XxJu4KI2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZWzMDILnYecp",
      "metadata": {
        "id": "ZWzMDILnYecp"
      },
      "outputs": [],
      "source": [
        "def load_combined_math_datasets(sample_size=10000):\n",
        "    \"\"\"IMPROVED: Load multiple math datasets for better training signal\"\"\"\n",
        "    print(f\"Loading multiple math datasets (up to {sample_size} examples total)...\")\n",
        "    datasets = []\n",
        "\n",
        "    # 1. Load NuminaMath dataset (primary source)\n",
        "    try:\n",
        "        numina_dataset = load_dataset(\"PrimeIntellect/NuminaMath-QwQ-CoT-5M\")\n",
        "        # Sample more examples\n",
        "        sampled_numina = numina_dataset[\"train\"].shuffle(seed=42).select(\n",
        "            range(min(5000, len(numina_dataset[\"train\"])))\n",
        "        )\n",
        "        datasets.append(sampled_numina)\n",
        "        print(f\"Added {len(sampled_numina)} examples from NuminaMath\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading NuminaMath: {e}\")\n",
        "\n",
        "    # 2. Add GSM8K training data for better alignment with evaluation\n",
        "    try:\n",
        "        gsm8k_train = load_dataset(\"gsm8k\", \"main\")[\"train\"]\n",
        "        # Prioritize GSM8K examples by taking more of them\n",
        "        sampled_gsm8k = gsm8k_train.shuffle(seed=42).select(\n",
        "            range(min(3000, len(gsm8k_train)))\n",
        "        )\n",
        "        datasets.append(sampled_gsm8k)\n",
        "        print(f\"Added {len(sampled_gsm8k)} examples from GSM8K train set\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading GSM8K: {e}\")\n",
        "\n",
        "    # If we have no datasets yet, fall back to the original function\n",
        "    if not datasets:\n",
        "        print(\"Falling back to original dataset loading function\")\n",
        "        try:\n",
        "            # Try to load the dataset\n",
        "            numina_dataset = load_dataset(\"PrimeIntellect/NuminaMath-QwQ-CoT-5M\")\n",
        "            sampled_data = numina_dataset[\"train\"].shuffle(seed=42).select(range(min(sample_size, len(numina_dataset[\"train\"]))))\n",
        "            print(f\"Successfully loaded {len(sampled_data)} examples from NuminaMath dataset\")\n",
        "            return sampled_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading PrimeIntellect dataset: {e}\")\n",
        "            try:\n",
        "                # Try loading GSM8k dataset as a fallback\n",
        "                print(\"Falling back to GSM8k dataset...\")\n",
        "                gsm8k_dataset = load_dataset(\"gsm8k\", \"main\")\n",
        "                print(f\"Successfully loaded GSM8k dataset with {len(gsm8k_dataset['train'])} examples\")\n",
        "                return gsm8k_dataset[\"train\"]\n",
        "            except Exception as e2:\n",
        "                print(f\"Error loading GSM8k dataset: {e2}\")\n",
        "                raise ValueError(\"Could not load any mathematics dataset. Please check your connection and try again.\")\n",
        "\n",
        "    # Combine all datasets and limit total size\n",
        "    combined_data = concatenate_datasets(datasets)\n",
        "    combined_data = combined_data.shuffle(seed=42)\n",
        "    if len(combined_data) > sample_size:\n",
        "        combined_data = combined_data.select(range(sample_size))\n",
        "\n",
        "    print(f\"Final combined dataset size: {len(combined_data)} examples\")\n",
        "\n",
        "    # Get a sample to inspect the data structure\n",
        "    sample = combined_data[0]\n",
        "    print(\"\\nSample data structure:\")\n",
        "    for key in sample:\n",
        "        if isinstance(sample[key], str):\n",
        "            print(f\"{key}: {sample[key][:100]}...\")\n",
        "        else:\n",
        "            print(f\"{key}: {sample[key]}\")\n",
        "\n",
        "    return combined_data\n",
        "\n",
        "\n",
        "def load_gsm8k_evaluation():\n",
        "    print(\"Loading GSM8k for evaluation...\")\n",
        "    gsm8k_dataset = load_dataset(\"gsm8k\", \"main\")\n",
        "    print(f\"GSM8k dataset loaded. Test size: {len(gsm8k_dataset['test'])}\")\n",
        "    return gsm8k_dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "primeintellect_data = load_combined_math_datasets(sample_size=8000)\n",
        "\n",
        "gsm8k_eval = load_gsm8k_evaluation()"
      ],
      "metadata": {
        "id": "AobQ8bQb8EC5"
      },
      "id": "AobQ8bQb8EC5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation Dataset"
      ],
      "metadata": {
        "id": "mXOudmNwpcsq"
      },
      "id": "mXOudmNwpcsq"
    },
    {
      "cell_type": "code",
      "source": [
        "def create_evaluation_set(gsm8k_eval, size=100):\n",
        "    \"\"\"IMPROVED: Create a larger evaluation set with better answer extraction\"\"\"\n",
        "    print(\"Creating evaluation dataset...\")\n",
        "\n",
        "    # Get samples from GSM8k test set\n",
        "    gsm8k_samples = gsm8k_eval.select(range(min(size, len(gsm8k_eval))))\n",
        "\n",
        "    # Format GSM8k samples\n",
        "    eval_set = []\n",
        "    for sample in gsm8k_samples:\n",
        "        # Find the answer in the solution\n",
        "        solution = sample[\"answer\"]\n",
        "\n",
        "        # Try to extract number after #### marker\n",
        "        hash_matches = re.findall(r\"####\\s*([-+]?\\d*\\.?\\d+)\", solution)\n",
        "        if hash_matches:\n",
        "            answer_value = hash_matches[-1].strip()\n",
        "        else:\n",
        "            # Fall back to extracting the last number in the solution\n",
        "            numbers = re.findall(r\"([-+]?\\d*\\.?\\d+)\", solution)\n",
        "            if numbers:\n",
        "                answer_value = numbers[-1].strip()\n",
        "            else:\n",
        "                # Last resort - use the text after #### marker\n",
        "                parts = solution.split(\"####\")\n",
        "                if len(parts) > 1:\n",
        "                    answer_value = parts[-1].strip()\n",
        "                else:\n",
        "                    answer_value = solution.strip().split(\"\\n\")[-1].strip()\n",
        "\n",
        "        eval_set.append({\n",
        "            \"category\": \"math_reasoning\",\n",
        "            \"question\": sample[\"question\"],\n",
        "            \"solution\": solution,\n",
        "            \"answer\": answer_value\n",
        "        })\n",
        "\n",
        "    # Save the evaluation set for consistency\n",
        "    with open(\"evaluation_set.json\", \"w\") as f:\n",
        "        json.dump(eval_set, f)\n",
        "\n",
        "    print(f\"Created evaluation set with {len(eval_set)} samples\")\n",
        "    return eval_set\n",
        "\n",
        "# Create the evaluation set\n",
        "evaluation_set = create_evaluation_set(gsm8k_eval, size=100)"
      ],
      "metadata": {
        "id": "uJ_hczT9pb7q"
      },
      "id": "uJ_hczT9pb7q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Evaluation Functions"
      ],
      "metadata": {
        "id": "ByR2XK3_B9XY"
      },
      "id": "ByR2XK3_B9XY"
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_and_tokenizer(model_name, load_in_4bit=True):\n",
        "    \"\"\"Load model and tokenizer with 4-bit quantization for reduced memory usage\"\"\"\n",
        "    print(f\"Loading {model_name} with 4-bit quantization...\")\n",
        "\n",
        "    # Configure quantization with additional CPU offloading parameters\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=load_in_4bit,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Handle tokenizer peculiarities\n",
        "    if not tokenizer.pad_token:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    print(\"Loading with CPU offloading enabled for memory efficiency...\")\n",
        "\n",
        "    # Check available GPU memory and determine if we need disk offloading\n",
        "    try:\n",
        "        free_in_GB = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        max_memory = {0: f\"{int(free_in_GB * 0.85)}GB\"}\n",
        "        print(f\"GPU memory available: {free_in_GB:.2f} GB, allocating: {max_memory}\")\n",
        "    except:\n",
        "        max_memory = None\n",
        "        print(\"Could not determine GPU memory, using default allocation\")\n",
        "\n",
        "    # IMPROVED: Better error handling during model loading\n",
        "    try:\n",
        "        # First try with 4-bit quantization\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=\"auto\",\n",
        "            quantization_config=quantization_config,\n",
        "            torch_dtype=torch.float16,\n",
        "            offload_folder=\"offload_folder\",\n",
        "            max_memory=max_memory,\n",
        "            offload_state_dict=True\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Initial loading attempt failed: {e}\")\n",
        "        print(\"Trying alternative loading strategy...\")\n",
        "        try:\n",
        "            # Try 8-bit quantization if 4-bit fails\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                device_map=\"auto\",\n",
        "                load_in_8bit=True,\n",
        "                torch_dtype=torch.float16,\n",
        "                offload_folder=\"offload_folder\"\n",
        "            )\n",
        "        except Exception as e2:\n",
        "            print(f\"8-bit loading also failed: {e2}\")\n",
        "            print(\"Trying with minimal configuration...\")\n",
        "            # Last resort - try loading with most conservative settings\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                device_map=\"auto\",\n",
        "                torch_dtype=torch.float16,\n",
        "                low_cpu_mem_usage=True\n",
        "            )\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "def format_math_prompt(question):\n",
        "    \"\"\"Format math question with improved prompt engineering\"\"\"\n",
        "    return f\"\"\"Solve this math problem by breaking it down into small, logical steps.\n",
        "\n",
        "Problem: {question}\n",
        "\n",
        "Follow these steps:\n",
        "1. Understand what the problem is asking for\n",
        "2. Identify the key variables and relationships\n",
        "3. Plan your approach step-by-step\n",
        "4. Execute each calculation carefully, showing your work\n",
        "5. Check your answer for reasonableness\n",
        "6. State the final numerical answer after ####\n",
        "\n",
        "Remember to maintain clear reasoning throughout and verify your calculations.\n",
        "\"\"\"\n",
        "\n",
        "def get_chat_template(tokenizer, model_name):\n",
        "    \"\"\"Get appropriate chat template for the model\"\"\"\n",
        "    if hasattr(tokenizer, 'chat_template') and tokenizer.chat_template:\n",
        "        template = Template(tokenizer.chat_template)\n",
        "        return template\n",
        "\n",
        "    # Default template for models without one\n",
        "    if \"mistral\" in model_name.lower():\n",
        "        template_str = \"<s>[INST] {{ messages[0]['content'] }} [/INST] {{ messages[1]['content'] }}</s>\"\n",
        "    else:\n",
        "        template_str = \"<|im_start|>user\\n{{ messages[0]['content'] }}<|im_end|>\\n<|im_start|>assistant\\n{{ messages[1]['content'] }}<|im_end|>\"\n",
        "\n",
        "    return Template(template_str)\n",
        "\n",
        "# IMPROVED: Enhanced answer extraction with multiple strategies\n",
        "def extract_answer(response, question_type):\n",
        "    \"\"\"Extract the final answer from model responses with improved pattern matching\"\"\"\n",
        "    if question_type == \"math_reasoning\":\n",
        "        # Try multiple extraction strategies in order of reliability\n",
        "\n",
        "        # 1. Find answer after #### marker (most reliable)\n",
        "        hash_match = re.search(r\"#{3,}\\s*([-+]?\\d*\\.?\\d+)\", response, re.DOTALL)\n",
        "        if hash_match:\n",
        "            return hash_match.group(1).strip()\n",
        "\n",
        "        # 2. Look for explicit \"The answer is X\" pattern\n",
        "        answer_match = re.search(r\"(?:the\\s+answer\\s+is|final\\s+answer\\s+is|answer\\s*[=:]\\s*)([-+]?\\d*\\.?\\d+)\",\n",
        "                                response.lower(), re.DOTALL)\n",
        "        if answer_match:\n",
        "            return answer_match.group(1).strip()\n",
        "\n",
        "        # 3. Look for \"Therefore\" pattern\n",
        "        therefore_match = re.search(r\"(?:therefore|thus|hence|so),?\\s*([-+]?\\d*\\.?\\d+)\",\n",
        "                                  response.lower(), re.DOTALL)\n",
        "        if therefore_match:\n",
        "            return therefore_match.group(1).strip()\n",
        "\n",
        "        # 4. Fallback to last number in the response\n",
        "        numbers = re.findall(r\"[-+]?\\d*\\.?\\d+\", response)\n",
        "        if numbers:\n",
        "            return numbers[-1]\n",
        "\n",
        "        return response.strip()\n",
        "    else:\n",
        "        # For other question types, just return the last sentence or phrase\n",
        "        response = response.strip()\n",
        "        sentences = response.split(\".\")\n",
        "        if sentences:\n",
        "            last_sentence = sentences[-1].strip()\n",
        "            # If it's very long, take the last part\n",
        "            if len(last_sentence) > 50:\n",
        "                last_sentence = last_sentence[-50:].strip()\n",
        "            return last_sentence\n",
        "\n",
        "        # Fallback to returning a short version of the response\n",
        "        if len(response) > 50:\n",
        "            return response[-50:].strip()\n",
        "        return response"
      ],
      "metadata": {
        "id": "PR5pZT465B5e"
      },
      "id": "PR5pZT465B5e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "631c6fd1",
      "metadata": {
        "id": "631c6fd1"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"NLP_ass3 Improved\n",
        "\n",
        "Enhancements to fine-tuning process for better math reasoning abilities\n",
        "\"\"\"\n",
        "\n",
        "# Keeping the original imports and setup\n",
        "import random\n",
        "import json\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import gc\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset, Dataset, concatenate_datasets\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer\n",
        "import re\n",
        "from jinja2 import Template\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from huggingface_hub import login\n",
        "login()\n",
        "\n",
        "# Random seed for reproducibility\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Improved memory clearing function\"\"\"\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "    print(\"Memory cleared\")\n",
        "\n",
        "# Set environment variables to limit memory usage\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
        "\n",
        "# Clear all memory before starting\n",
        "clear_memory()\n",
        "\n",
        "def load_combined_math_datasets(sample_size=10000):\n",
        "    \"\"\"IMPROVED: Load multiple math datasets for better training signal\"\"\"\n",
        "    print(f\"Loading multiple math datasets (up to {sample_size} examples total)...\")\n",
        "    datasets = []\n",
        "\n",
        "    # 1. Load NuminaMath dataset (primary source)\n",
        "    try:\n",
        "        numina_dataset = load_dataset(\"PrimeIntellect/NuminaMath-QwQ-CoT-5M\")\n",
        "        # Sample more examples\n",
        "        sampled_numina = numina_dataset[\"train\"].shuffle(seed=42).select(\n",
        "            range(min(5000, len(numina_dataset[\"train\"])))\n",
        "        )\n",
        "        datasets.append(sampled_numina)\n",
        "        print(f\"Added {len(sampled_numina)} examples from NuminaMath\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading NuminaMath: {e}\")\n",
        "\n",
        "    # 2. Add GSM8K training data for better alignment with evaluation\n",
        "    try:\n",
        "        gsm8k_train = load_dataset(\"gsm8k\", \"main\")[\"train\"]\n",
        "        # Prioritize GSM8K examples by taking more of them\n",
        "        sampled_gsm8k = gsm8k_train.shuffle(seed=42).select(\n",
        "            range(min(3000, len(gsm8k_train)))\n",
        "        )\n",
        "        datasets.append(sampled_gsm8k)\n",
        "        print(f\"Added {len(sampled_gsm8k)} examples from GSM8K train set\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading GSM8K: {e}\")\n",
        "\n",
        "    # If we have no datasets yet, fall back to the original function\n",
        "    if not datasets:\n",
        "        print(\"Falling back to original dataset loading function\")\n",
        "        try:\n",
        "            # Try to load the dataset\n",
        "            numina_dataset = load_dataset(\"PrimeIntellect/NuminaMath-QwQ-CoT-5M\")\n",
        "            sampled_data = numina_dataset[\"train\"].shuffle(seed=42).select(range(min(sample_size, len(numina_dataset[\"train\"]))))\n",
        "            print(f\"Successfully loaded {len(sampled_data)} examples from NuminaMath dataset\")\n",
        "            return sampled_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading PrimeIntellect dataset: {e}\")\n",
        "            try:\n",
        "                # Try loading GSM8k dataset as a fallback\n",
        "                print(\"Falling back to GSM8k dataset...\")\n",
        "                gsm8k_dataset = load_dataset(\"gsm8k\", \"main\")\n",
        "                print(f\"Successfully loaded GSM8k dataset with {len(gsm8k_dataset['train'])} examples\")\n",
        "                return gsm8k_dataset[\"train\"]\n",
        "            except Exception as e2:\n",
        "                print(f\"Error loading GSM8k dataset: {e2}\")\n",
        "                raise ValueError(\"Could not load any mathematics dataset. Please check your connection and try again.\")\n",
        "\n",
        "    # Combine all datasets and limit total size\n",
        "    combined_data = concatenate_datasets(datasets)\n",
        "    combined_data = combined_data.shuffle(seed=42)\n",
        "    if len(combined_data) > sample_size:\n",
        "        combined_data = combined_data.select(range(sample_size))\n",
        "\n",
        "    print(f\"Final combined dataset size: {len(combined_data)} examples\")\n",
        "\n",
        "    # Get a sample to inspect the data structure\n",
        "    sample = combined_data[0]\n",
        "    print(\"\\nSample data structure:\")\n",
        "    for key in sample:\n",
        "        if isinstance(sample[key], str):\n",
        "            print(f\"{key}: {sample[key][:100]}...\")\n",
        "        else:\n",
        "            print(f\"{key}: {sample[key]}\")\n",
        "\n",
        "    return combined_data\n",
        "\n",
        "# Also load GSM8k for evaluation\n",
        "def load_gsm8k_evaluation():\n",
        "    print(\"Loading GSM8k for evaluation...\")\n",
        "    gsm8k_dataset = load_dataset(\"gsm8k\", \"main\")\n",
        "    print(f\"GSM8k dataset loaded. Test size: {len(gsm8k_dataset['test'])}\")\n",
        "    return gsm8k_dataset[\"test\"]\n",
        "\n",
        "# IMPROVED: Increase the sample size for better training signal\n",
        "primeintellect_data = load_combined_math_datasets(sample_size=8000)\n",
        "\n",
        "# Load GSM8k for evaluation\n",
        "gsm8k_eval = load_gsm8k_evaluation()\n",
        "\n",
        "def create_evaluation_set(gsm8k_eval, size=100):\n",
        "    \"\"\"IMPROVED: Create a larger evaluation set with better answer extraction\"\"\"\n",
        "    print(\"Creating evaluation dataset...\")\n",
        "\n",
        "    # Get samples from GSM8k test set - increased size for more reliable evaluation\n",
        "    gsm8k_samples = gsm8k_eval.select(range(min(size, len(gsm8k_eval))))\n",
        "\n",
        "    # Format GSM8k samples with improved answer extraction\n",
        "    eval_set = []\n",
        "    for sample in gsm8k_samples:\n",
        "        # Find the answer in the solution\n",
        "        solution = sample[\"answer\"]\n",
        "\n",
        "        # IMPROVED: More reliable answer extraction\n",
        "        # Try to extract number after #### marker\n",
        "        hash_matches = re.findall(r\"####\\s*([-+]?\\d*\\.?\\d+)\", solution)\n",
        "        if hash_matches:\n",
        "            answer_value = hash_matches[-1].strip()\n",
        "        else:\n",
        "            # Fall back to extracting the last number in the solution\n",
        "            numbers = re.findall(r\"([-+]?\\d*\\.?\\d+)\", solution)\n",
        "            if numbers:\n",
        "                answer_value = numbers[-1].strip()\n",
        "            else:\n",
        "                # Last resort - use the text after #### marker\n",
        "                parts = solution.split(\"####\")\n",
        "                if len(parts) > 1:\n",
        "                    answer_value = parts[-1].strip()\n",
        "                else:\n",
        "                    answer_value = solution.strip().split(\"\\n\")[-1].strip()\n",
        "\n",
        "        eval_set.append({\n",
        "            \"category\": \"math_reasoning\",\n",
        "            \"question\": sample[\"question\"],\n",
        "            \"solution\": solution,\n",
        "            \"answer\": answer_value\n",
        "        })\n",
        "\n",
        "    # Save the evaluation set for consistency\n",
        "    with open(\"evaluation_set.json\", \"w\") as f:\n",
        "        json.dump(eval_set, f)\n",
        "\n",
        "    print(f\"Created evaluation set with {len(eval_set)} samples\")\n",
        "    return eval_set\n",
        "\n",
        "# Create the evaluation set - IMPROVED: larger size\n",
        "evaluation_set = create_evaluation_set(gsm8k_eval, size=100)\n",
        "\n",
        "def load_model_and_tokenizer(model_name, load_in_4bit=True):\n",
        "    \"\"\"Load model and tokenizer with 4-bit quantization for reduced memory usage\"\"\"\n",
        "    print(f\"Loading {model_name} with 4-bit quantization...\")\n",
        "\n",
        "    # Configure quantization with additional CPU offloading parameters\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=load_in_4bit,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Handle tokenizer peculiarities\n",
        "    if not tokenizer.pad_token:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    print(\"Loading with CPU offloading enabled for memory efficiency...\")\n",
        "\n",
        "    # Check available GPU memory and determine if we need disk offloading\n",
        "    try:\n",
        "        free_in_GB = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        max_memory = {0: f\"{int(free_in_GB * 0.85)}GB\"}\n",
        "        print(f\"GPU memory available: {free_in_GB:.2f} GB, allocating: {max_memory}\")\n",
        "    except:\n",
        "        max_memory = None\n",
        "        print(\"Could not determine GPU memory, using default allocation\")\n",
        "\n",
        "    # IMPROVED: Better error handling during model loading\n",
        "    try:\n",
        "        # First try with 4-bit quantization\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=\"auto\",\n",
        "            quantization_config=quantization_config,\n",
        "            torch_dtype=torch.float16,\n",
        "            offload_folder=\"offload_folder\",\n",
        "            max_memory=max_memory,\n",
        "            offload_state_dict=True\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Initial loading attempt failed: {e}\")\n",
        "        print(\"Trying alternative loading strategy...\")\n",
        "        try:\n",
        "            # Try 8-bit quantization if 4-bit fails\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                device_map=\"auto\",\n",
        "                load_in_8bit=True,\n",
        "                torch_dtype=torch.float16,\n",
        "                offload_folder=\"offload_folder\"\n",
        "            )\n",
        "        except Exception as e2:\n",
        "            print(f\"8-bit loading also failed: {e2}\")\n",
        "            print(\"Trying with minimal configuration...\")\n",
        "            # Last resort - try loading with most conservative settings\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                device_map=\"auto\",\n",
        "                torch_dtype=torch.float16,\n",
        "                low_cpu_mem_usage=True\n",
        "            )\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# IMPROVED: Enhanced prompt engineering with more explicit reasoning guidance\n",
        "def format_math_prompt(question):\n",
        "    \"\"\"Format math question with improved prompt engineering\"\"\"\n",
        "    return f\"\"\"Solve this math problem by breaking it down into small, logical steps.\n",
        "\n",
        "Problem: {question}\n",
        "\n",
        "Follow these steps:\n",
        "1. Understand what the problem is asking for\n",
        "2. Identify the key variables and relationships\n",
        "3. Plan your approach step-by-step\n",
        "4. Execute each calculation carefully, showing your work\n",
        "5. Check your answer for reasonableness\n",
        "6. State the final numerical answer after ####\n",
        "\n",
        "Remember to maintain clear reasoning throughout and verify your calculations.\n",
        "\"\"\"\n",
        "\n",
        "def get_chat_template(tokenizer, model_name):\n",
        "    \"\"\"Get appropriate chat template for the model\"\"\"\n",
        "    if hasattr(tokenizer, 'chat_template') and tokenizer.chat_template:\n",
        "        template = Template(tokenizer.chat_template)\n",
        "        return template\n",
        "\n",
        "    # Default template for models without one\n",
        "    if \"mistral\" in model_name.lower():\n",
        "        template_str = \"<s>[INST] {{ messages[0]['content'] }} [/INST] {{ messages[1]['content'] }}</s>\"\n",
        "    else:\n",
        "        template_str = \"<|im_start|>user\\n{{ messages[0]['content'] }}<|im_end|>\\n<|im_start|>assistant\\n{{ messages[1]['content'] }}<|im_end|>\"\n",
        "\n",
        "    return Template(template_str)\n",
        "\n",
        "# IMPROVED: Enhanced answer extraction with multiple strategies\n",
        "def extract_answer(response, question_type):\n",
        "    \"\"\"Extract the final answer from model responses with improved pattern matching\"\"\"\n",
        "    if question_type == \"math_reasoning\":\n",
        "        # Try multiple extraction strategies in order of reliability\n",
        "\n",
        "        # 1. Find answer after #### marker (most reliable)\n",
        "        hash_match = re.search(r\"#{3,}\\s*([-+]?\\d*\\.?\\d+)\", response, re.DOTALL)\n",
        "        if hash_match:\n",
        "            return hash_match.group(1).strip()\n",
        "\n",
        "        # 2. Look for explicit \"The answer is X\" pattern\n",
        "        answer_match = re.search(r\"(?:the\\s+answer\\s+is|final\\s+answer\\s+is|answer\\s*[=:]\\s*)([-+]?\\d*\\.?\\d+)\",\n",
        "                                response.lower(), re.DOTALL)\n",
        "        if answer_match:\n",
        "            return answer_match.group(1).strip()\n",
        "\n",
        "        # 3. Look for \"Therefore\" pattern\n",
        "        therefore_match = re.search(r\"(?:therefore|thus|hence|so),?\\s*([-+]?\\d*\\.?\\d+)\",\n",
        "                                  response.lower(), re.DOTALL)\n",
        "        if therefore_match:\n",
        "            return therefore_match.group(1).strip()\n",
        "\n",
        "        # 4. Fallback to last number in the response\n",
        "        numbers = re.findall(r\"[-+]?\\d*\\.?\\d+\", response)\n",
        "        if numbers:\n",
        "            return numbers[-1]\n",
        "\n",
        "        return response.strip()\n",
        "    else:\n",
        "        # For other question types, just return the last sentence or phrase\n",
        "        response = response.strip()\n",
        "        sentences = response.split(\".\")\n",
        "        if sentences:\n",
        "            last_sentence = sentences[-1].strip()\n",
        "            # If it's very long, take the last part\n",
        "            if len(last_sentence) > 50:\n",
        "                last_sentence = last_sentence[-50:].strip()\n",
        "            return last_sentence\n",
        "\n",
        "        # Fallback to returning a short version of the response\n",
        "        if len(response) > 50:\n",
        "            return response[-50:].strip()\n",
        "        return response\n",
        "\n",
        "# IMPROVED: Better evaluation with more robust error handling\n",
        "def evaluate_model(model, tokenizer, evaluation_data, model_name):\n",
        "    \"\"\"Evaluate model on the evaluation set with improved generation parameters\"\"\"\n",
        "    template = get_chat_template(tokenizer, model_name)\n",
        "    results = []\n",
        "\n",
        "    # Track successful and failed evaluations\n",
        "    success_count = 0\n",
        "    failure_count = 0\n",
        "\n",
        "    for idx, item in enumerate(tqdm(evaluation_data, desc=f\"Evaluating {model_name}\")):\n",
        "        try:\n",
        "            question = item[\"question\"]\n",
        "            true_answer = item[\"answer\"]\n",
        "            category = item[\"category\"]\n",
        "\n",
        "            # IMPROVED: Use enhanced prompt format\n",
        "            prompt = format_math_prompt(question)\n",
        "\n",
        "            # Format message using template\n",
        "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "            # Some models use chat templating differently\n",
        "            try:\n",
        "                input_text = template.render(messages=messages)\n",
        "            except:\n",
        "                # Fallback for models with different templating\n",
        "                if \"mistral\" in model_name.lower():\n",
        "                    input_text = f\"<s>[INST] {prompt} [/INST]\"\n",
        "                else:\n",
        "                    input_text = f\"<|im_start|>user\\n{prompt}<|im_end|>\"\n",
        "\n",
        "            # Tokenize with proper handling\n",
        "            inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
        "            input_ids = inputs.input_ids.to(model.device)\n",
        "\n",
        "            # IMPROVED: Enhanced generation parameters for math reasoning\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    input_ids,\n",
        "                    max_new_tokens=512,  # Longer generation for complete reasoning\n",
        "                    temperature=0.2,     # Lower temperature for more deterministic outputs\n",
        "                    top_p=0.92,          # Narrower distribution for higher quality\n",
        "                    do_sample=True,      # Still use sampling for some diversity\n",
        "                    num_beams=2,         # Simple beam search for better outputs\n",
        "                    repetition_penalty=1.1, # Discourage repetitive loops\n",
        "                    pad_token_id=tokenizer.pad_token_id\n",
        "                )\n",
        "\n",
        "            # Decode the full response\n",
        "            full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            # Extract just the model's response (not the prompt)\n",
        "            response = full_output[len(tokenizer.decode(input_ids[0], skip_special_tokens=True)):].strip()\n",
        "\n",
        "            # Extract the answer with improved extraction\n",
        "            extracted_answer = extract_answer(response, category)\n",
        "\n",
        "            # IMPROVED: More robust correctness checking\n",
        "            is_correct = False\n",
        "            if category == \"math_reasoning\":\n",
        "                # Clean up numbers for comparison - handle commas, currency symbols\n",
        "                extracted_clean = re.sub(r'[^\\d.-]', '', extracted_answer)\n",
        "                true_clean = re.sub(r'[^\\d.-]', '', true_answer)\n",
        "\n",
        "                try:\n",
        "                    # Compare as floats with small tolerance for rounding errors\n",
        "                    extracted_float = float(extracted_clean)\n",
        "                    true_float = float(true_clean)\n",
        "                    is_correct = abs(extracted_float - true_float) < 1e-6\n",
        "                except:\n",
        "                    # If conversion fails, fall back to string comparison\n",
        "                    is_correct = extracted_clean == true_clean\n",
        "\n",
        "            results.append({\n",
        "                \"category\": category,\n",
        "                \"question\": question,\n",
        "                \"true_answer\": true_answer,\n",
        "                \"model_response\": response,\n",
        "                \"extracted_answer\": extracted_answer,\n",
        "                \"is_correct\": is_correct\n",
        "            })\n",
        "\n",
        "            success_count += 1\n",
        "\n",
        "            # Show example of first few evaluations\n",
        "            if idx < 2:  # Show first two examples\n",
        "                print(f\"\\nExample {idx+1}:\")\n",
        "                print(f\"Question: {question[:100]}...\" if len(question) > 100 else f\"Question: {question}\")\n",
        "                print(f\"True answer: {true_answer}\")\n",
        "                print(f\"Model extracted answer: {extracted_answer}\")\n",
        "                print(f\"Correct: {'✓' if is_correct else '✗'}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating example {idx}: {e}\")\n",
        "            failure_count += 1\n",
        "            continue\n",
        "\n",
        "    # Calculate overall accuracy\n",
        "    correct = sum(1 for r in results if r[\"is_correct\"])\n",
        "    accuracy = correct / len(results) if results else 0\n",
        "\n",
        "    print(f\"\\n{model_name} Evaluation Results:\")\n",
        "    print(f\"Overall Accuracy: {accuracy:.4f} ({correct}/{len(results)})\")\n",
        "    print(f\"Successful evaluations: {success_count}, Failed: {failure_count}\")\n",
        "\n",
        "    return {\n",
        "        \"model_name\": model_name,\n",
        "        \"overall_accuracy\": accuracy,\n",
        "        \"detailed_results\": results\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Process PrimeIntellect Dataset for Training"
      ],
      "metadata": {
        "id": "bb-iW9IQCLVs"
      },
      "id": "bb-iW9IQCLVs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MYS6k8ODiRwf",
      "metadata": {
        "id": "MYS6k8ODiRwf"
      },
      "outputs": [],
      "source": [
        "def process_numina_data_for_training(data, tokenizer, model_name):\n",
        "    \"\"\"Process the dataset for training with improved formatting and memory management\"\"\"\n",
        "    print(\"Processing dataset for training...\")\n",
        "\n",
        "    template = get_chat_template(tokenizer, model_name)\n",
        "    processed_data = []\n",
        "\n",
        "    # Identify correct field names based on dataset structure\n",
        "    sample = data[0]\n",
        "\n",
        "    # IMPROVED: More robust field detection logic\n",
        "    if 'prompt' in sample and 'response' in sample:\n",
        "        question_key, answer_key = 'prompt', 'response'\n",
        "    elif 'question' in sample and 'answer' in sample:\n",
        "        question_key, answer_key = 'question', 'answer'\n",
        "    elif 'input' in sample and 'output' in sample:\n",
        "        question_key, answer_key = 'input', 'output'\n",
        "    else:\n",
        "        # Make an educated guess based on available fields\n",
        "        keys = list(sample.keys())\n",
        "        str_keys = [k for k in keys if isinstance(sample[k], str)]\n",
        "\n",
        "        if len(str_keys) >= 2:\n",
        "            question_key, answer_key = str_keys[0], str_keys[1]\n",
        "            print(f\"Using inferred fields - Question: '{question_key}', Answer: '{answer_key}'\")\n",
        "        else:\n",
        "            raise ValueError(f\"Cannot identify question and answer fields. Keys: {keys}\")\n",
        "\n",
        "    print(f\"Using fields - Question: '{question_key}', Answer: '{answer_key}'\")\n",
        "\n",
        "    # Process the data in smaller chunks for better memory management\n",
        "    chunk_size = 100\n",
        "    for chunk_start in range(0, len(data), chunk_size):\n",
        "        chunk_end = min(chunk_start + chunk_size, len(data))\n",
        "        print(f\"Processing chunk {chunk_start} to {chunk_end-1}...\")\n",
        "\n",
        "        for i in range(chunk_start, chunk_end):\n",
        "            try:\n",
        "                item = data[i]\n",
        "                question = item[question_key]\n",
        "                answer = item[answer_key]\n",
        "\n",
        "                # Skip invalid entries\n",
        "                if not isinstance(question, str) or not isinstance(answer, str):\n",
        "                    print(f\"Skipping item {i}: question or answer is not a string\")\n",
        "                    continue\n",
        "\n",
        "                # IMPROVED: Enhanced prompt formatting\n",
        "                formatted_question = format_math_prompt(question)\n",
        "\n",
        "                # IMPROVED: Ensure answers end with the output marker if not present\n",
        "                if '####' not in answer:\n",
        "                    # Try to find the final answer\n",
        "                    numbers = re.findall(r\"([-+]?\\d*\\.?\\d+)\", answer)\n",
        "                    if numbers:\n",
        "                        final_number = numbers[-1].strip()\n",
        "                        # Only add #### if not already at the end\n",
        "                        if not answer.strip().endswith(final_number):\n",
        "                            answer = answer.strip() + f\"\\n\\n#### {final_number}\"\n",
        "\n",
        "                # Format into chat template\n",
        "                try:\n",
        "                    messages = [\n",
        "                        {\"role\": \"user\", \"content\": formatted_question},\n",
        "                        {\"role\": \"assistant\", \"content\": answer}\n",
        "                    ]\n",
        "                    formatted_text = template.render(messages=messages)\n",
        "                except Exception as e:\n",
        "                    # Fallback for models with different templating\n",
        "                    if \"mistral\" in model_name.lower():\n",
        "                        formatted_text = f\"<s>[INST] {formatted_question} [/INST] {answer}</s>\"\n",
        "                    else:\n",
        "                        formatted_text = f\"<|im_start|>user\\n{formatted_question}<|im_end|>\\n<|im_start|>assistant\\n{answer}<|im_end|>\"\n",
        "\n",
        "                # Tokenize with truncation to control length - IMPROVED: longer context\n",
        "                tokenized = tokenizer(formatted_text, truncation=True, max_length=1536)\n",
        "\n",
        "                processed_data.append({\n",
        "                    \"input_ids\": tokenized[\"input_ids\"],\n",
        "                    \"attention_mask\": tokenized[\"attention_mask\"],\n",
        "                })\n",
        "\n",
        "                # Show sample for debugging\n",
        "                if i == chunk_start:\n",
        "                    print(f\"\\nSample processed data (item {i}):\")\n",
        "                    print(f\"Original question: {question[:100]}...\" if len(question) > 100 else f\"Original question: {question}\")\n",
        "                    print(f\"Original answer: {answer[:100]}...\" if len(answer) > 100 else f\"Original answer: {answer}\")\n",
        "                    print(f\"Tokenized length: {len(tokenized['input_ids'])}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing item {i}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Clear memory after each chunk\n",
        "        clear_memory()\n",
        "        print(f\"Processed {len(processed_data)} examples so far\")\n",
        "\n",
        "    # Final check\n",
        "    if len(processed_data) == 0:\n",
        "        raise ValueError(\"No examples were successfully processed. Please check the dataset format.\")\n",
        "\n",
        "    print(f\"Successfully processed {len(processed_data)} examples\")\n",
        "\n",
        "    # Create dataset-like structure\n",
        "    return Dataset.from_dict({\n",
        "        \"input_ids\": [item[\"input_ids\"] for item in processed_data],\n",
        "        \"attention_mask\": [item[\"attention_mask\"] for item in processed_data]\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_labels_from_input_ids(batched_input_ids, tokenizer, model_name):\n",
        "    \"\"\"Create labels for training where we only want to predict the assistant's response\"\"\"\n",
        "    labels = []\n",
        "\n",
        "    for input_ids in batched_input_ids:\n",
        "        # Convert input_ids to string\n",
        "        full_text = tokenizer.decode(input_ids)\n",
        "\n",
        "        # Identify assistant's part based on model\n",
        "        if \"mistral\" in model_name.lower():\n",
        "            # For Mistral, find text after [/INST]\n",
        "            parts = full_text.split(\"[/INST]\")\n",
        "            if len(parts) > 1:\n",
        "                user_text = parts[0] + \"[/INST]\"\n",
        "            else:\n",
        "                user_text = full_text\n",
        "        else:\n",
        "            # For other models, find text between markers\n",
        "            parts = re.split(r\"<\\|im_start\\|>assistant\", full_text)\n",
        "            if len(parts) > 1:\n",
        "                user_text = parts[0] + \"<|im_start|>assistant\"\n",
        "            else:\n",
        "                user_text = full_text\n",
        "\n",
        "        # Tokenize user part\n",
        "        user_ids = tokenizer(user_text, add_special_tokens=False)[\"input_ids\"]\n",
        "\n",
        "        # Create labels with -100 for user part (to ignore in loss calculation)\n",
        "        label = [-100] * min(len(user_ids), len(input_ids))\n",
        "\n",
        "        # Fill the rest with actual values\n",
        "        if len(user_ids) < len(input_ids):\n",
        "            label.extend(input_ids[len(user_ids):])\n",
        "\n",
        "        labels.append(label)\n",
        "\n",
        "    return labels"
      ],
      "metadata": {
        "id": "AVAua1NpWcSE"
      },
      "execution_count": null,
      "outputs": [],
      "id": "AVAua1NpWcSE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Two-Stage Fine-tuning - SFT followed by RLHF"
      ],
      "metadata": {
        "id": "2D9_EiZYCkTS"
      },
      "id": "2D9_EiZYCkTS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yyFrEPnXfm5m",
      "metadata": {
        "id": "yyFrEPnXfm5m"
      },
      "outputs": [],
      "source": [
        "def fine_tune_model(model_name, training_data, tokenizer=None, output_dir=None):\n",
        "    \"\"\"Fine-tuning process with improved LoRA configuration and training parameters\"\"\"\n",
        "    if output_dir is None:\n",
        "        output_dir = f\"fine_tuned_{model_name.split('/')[-1].lower().replace('-', '_')}\"\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Starting fine-tuning for {model_name}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # Configure quantization for memory efficiency\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    # Load model and tokenizer if not provided\n",
        "    if tokenizer is None:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        if not tokenizer.pad_token_id:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    print(\"Loading model with memory efficiency settings...\")\n",
        "    try:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\",\n",
        "            torch_dtype=torch.bfloat16\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading with 4-bit: {e}\")\n",
        "        try:\n",
        "            # Fallback to 8-bit if 4-bit fails\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                load_in_8bit=True,\n",
        "                device_map=\"auto\",\n",
        "                torch_dtype=torch.bfloat16\n",
        "            )\n",
        "        except Exception as e2:\n",
        "            print(f\"Error loading with 8-bit: {e2}\")\n",
        "            # Last resort - try loading with minimal settings\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                device_map=\"auto\",\n",
        "                torch_dtype=torch.bfloat16,\n",
        "                low_cpu_mem_usage=True\n",
        "            )\n",
        "\n",
        "    # Prepare model for training\n",
        "    model.gradient_checkpointing_enable()\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    # Process data if needed\n",
        "    if not isinstance(training_data, Dataset):\n",
        "        training_data = process_numina_data_for_training(training_data, tokenizer, model_name)\n",
        "\n",
        "    # Add labels to dataset\n",
        "    def add_labels(examples):\n",
        "        examples[\"labels\"] = create_labels_from_input_ids(examples[\"input_ids\"], tokenizer, model_name)\n",
        "        return examples\n",
        "\n",
        "    print(\"Preparing dataset with labels...\")\n",
        "    training_data = training_data.map(add_labels, batched=True)\n",
        "\n",
        "    # IMPROVED: Enhanced LoRA configuration for better math reasoning\n",
        "    # Higher rank and alpha values for more expressive adaptation\n",
        "    peft_config = LoraConfig(\n",
        "        r=32,              # IMPROVED: Increased from 8 to 32 for more capacity\n",
        "        lora_alpha=64,     # IMPROVED: Increased from 16 to 64 for stronger updates\n",
        "        lora_dropout=0.1,  # IMPROVED: Increased dropout for better generalization\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\n",
        "            # IMPROVED: Target more modules for better adaptation\n",
        "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "            \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, peft_config)\n",
        "    print(f\"Model prepared with LoRA adapters - rank={peft_config.r}, alpha={peft_config.lora_alpha}\")\n",
        "\n",
        "    # IMPROVED: Better training settings\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=2,             # IMPROVED: Train for 2 epochs instead of steps\n",
        "        per_device_train_batch_size=2,  # Small batch size but for more epochs\n",
        "        gradient_accumulation_steps=8,  # IMPROVED: Increased for larger effective batch\n",
        "        learning_rate=1e-4,             # IMPROVED: Slightly lower but more stable\n",
        "        weight_decay=0.05,              # IMPROVED: Higher weight decay for regularization\n",
        "        warmup_ratio=0.05,              # IMPROVED: Longer warmup phase\n",
        "        max_grad_norm=0.5,              # IMPROVED: Higher for stability\n",
        "        logging_steps=10,\n",
        "        save_strategy=\"epoch\",          # IMPROVED: Save each epoch\n",
        "        save_total_limit=3,             # Keep top 3 checkpoints\n",
        "        optim=\"paged_adamw_32bit\",      # Memory-efficient optimizer\n",
        "        fp16=True,                      # Mixed precision\n",
        "        gradient_checkpointing=True,    # Memory efficiency\n",
        "        lr_scheduler_type=\"cosine\",     # IMPROVED: Better scheduler\n",
        "        report_to=\"none\",               # Disable wandb/tensorboard to save memory\n",
        "    )\n",
        "\n",
        "    # Create a better data collator\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False  # We're doing causal language modeling\n",
        "    )\n",
        "\n",
        "    # Create trainer with improved collator\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=training_data,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # Save model\n",
        "    model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    print(f\"Model saved to {output_dir}\")\n",
        "\n",
        "    # Clear memory\n",
        "    del model\n",
        "    del trainer\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return output_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "479baff4",
      "metadata": {
        "id": "479baff4"
      },
      "source": [
        "#Evaluate Fine-tuned Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cGeKQrgRfpcC",
      "metadata": {
        "id": "cGeKQrgRfpcC"
      },
      "outputs": [],
      "source": [
        "def evaluate_finetuned_model(base_model_name, adapter_path, evaluation_set):\n",
        "    \"\"\"Evaluate a fine-tuned model with proper error handling\"\"\"\n",
        "    print(f\"\\nEvaluating fine-tuned model from {adapter_path}...\")\n",
        "\n",
        "    # Make sure the adapter path exists\n",
        "    if not os.path.exists(adapter_path):\n",
        "        print(f\"Error: Adapter path {adapter_path} not found\")\n",
        "        # Try to find any checkpoint\n",
        "        import glob\n",
        "        possible_paths = glob.glob(f\"{os.path.dirname(adapter_path)}/checkpoint-*\")\n",
        "        if possible_paths:\n",
        "            adapter_path = possible_paths[-1]  # Use the latest checkpoint\n",
        "            print(f\"Using alternative adapter path: {adapter_path}\")\n",
        "        else:\n",
        "            print(\"No checkpoints found.\")\n",
        "            return None\n",
        "\n",
        "    try:\n",
        "        # Load tokenizer from base model\n",
        "        print(\"Loading tokenizer...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "        if not tokenizer.pad_token:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        # First try to load a checkpoint with the whole model (some checkpoints save full model)\n",
        "        try:\n",
        "            print(f\"Attempting to load as full model from {adapter_path}...\")\n",
        "            adapter_model = AutoModelForCausalLM.from_pretrained(\n",
        "                adapter_path,\n",
        "                device_map=\"auto\",\n",
        "                load_in_4bit=True,\n",
        "                torch_dtype=torch.float16\n",
        "            )\n",
        "            print(\"Successfully loaded as full model\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load as full model: {e}\")\n",
        "            print(\"Loading base model first...\")\n",
        "\n",
        "            # Load base model with minimal settings to save memory\n",
        "            base_model = AutoModelForCausalLM.from_pretrained(\n",
        "                base_model_name,\n",
        "                device_map=\"auto\",\n",
        "                load_in_4bit=True,\n",
        "                torch_dtype=torch.float16\n",
        "            )\n",
        "\n",
        "            # Load adapter\n",
        "            print(f\"Loading adapter from {adapter_path}...\")\n",
        "            adapter_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "\n",
        "            # Delete base model reference to save memory\n",
        "            del base_model\n",
        "            clear_memory()\n",
        "\n",
        "        # Evaluate model\n",
        "        results = evaluate_model(adapter_model, tokenizer, evaluation_set, f\"Fine-tuned {base_model_name}\")\n",
        "\n",
        "        # Save results\n",
        "        with open(os.path.join(os.path.dirname(adapter_path), \"ft_eval_results.json\"), \"w\") as f:\n",
        "            json.dump(results, f)\n",
        "\n",
        "        # Free memory\n",
        "        del adapter_model\n",
        "        clear_memory()\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during fine-tuned model evaluation: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Compare Results and Report"
      ],
      "metadata": {
        "id": "8XXS4QTkCxSK"
      },
      "id": "8XXS4QTkCxSK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77f96057",
      "metadata": {
        "id": "77f96057"
      },
      "outputs": [],
      "source": [
        "def find_best_checkpoint(output_dir):\n",
        "    \"\"\"Find the best checkpoint in the output directory\"\"\"\n",
        "    import glob\n",
        "    import os\n",
        "\n",
        "    print(f\"Looking for checkpoints in {output_dir}...\")\n",
        "\n",
        "    # Look for checkpoints\n",
        "    checkpoints = glob.glob(f\"{output_dir}/checkpoint-*\")\n",
        "\n",
        "    if not checkpoints:\n",
        "        print(\"No checkpoints found, using the main output directory\")\n",
        "        return output_dir\n",
        "\n",
        "    # Find checkpoint with trainer_state.json to get loss information\n",
        "    valid_checkpoints = []\n",
        "    for cp in checkpoints:\n",
        "        state_file = os.path.join(cp, \"trainer_state.json\")\n",
        "        if os.path.exists(state_file):\n",
        "            try:\n",
        "                with open(state_file, \"r\") as f:\n",
        "                    state = json.load(f)\n",
        "                if \"log_history\" in state and state[\"log_history\"]:\n",
        "                    # Get the last loss\n",
        "                    last_loss = float(\"inf\")\n",
        "                    for entry in reversed(state[\"log_history\"]):\n",
        "                        if \"loss\" in entry:\n",
        "                            last_loss = entry[\"loss\"]\n",
        "                            break\n",
        "                    valid_checkpoints.append((cp, last_loss))\n",
        "            except:\n",
        "                # If we can't read the file, just use the checkpoint\n",
        "                valid_checkpoints.append((cp, float(\"inf\")))\n",
        "\n",
        "    if not valid_checkpoints:\n",
        "        # If no valid checkpoints with loss info, sort by step number\n",
        "        checkpoints.sort(key=lambda x: int(x.split(\"-\")[-1]))\n",
        "        print(f\"Using latest checkpoint by step: {checkpoints[-1]}\")\n",
        "        return checkpoints[-1]\n",
        "\n",
        "    # Otherwise use the checkpoint with the lowest loss\n",
        "    valid_checkpoints.sort(key=lambda x: x[1])\n",
        "    print(f\"Using best checkpoint with loss {valid_checkpoints[0][1]}: {valid_checkpoints[0][0]}\")\n",
        "    return valid_checkpoints[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_results(base_results, ft_results, output_path=\"model_comparison.png\"):\n",
        "    \"\"\"Create visualization comparing base and fine-tuned model performance\"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # If we don't have both results, skip visualization\n",
        "    if not base_results or not ft_results:\n",
        "        print(\"Cannot create visualization - missing results\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        base_acc = base_results[\"overall_accuracy\"]\n",
        "        ft_acc = ft_results[\"overall_accuracy\"]\n",
        "\n",
        "        # Calculate improvement\n",
        "        abs_improvement = ft_acc - base_acc\n",
        "        rel_improvement = (abs_improvement / base_acc * 100) if base_acc > 0 else float('inf')\n",
        "\n",
        "        print(f\"\\n===== MODEL COMPARISON =====\")\n",
        "        print(f\"Base model accuracy: {base_acc:.4f}\")\n",
        "        print(f\"Fine-tuned model accuracy: {ft_acc:.4f}\")\n",
        "        print(f\"Absolute improvement: {abs_improvement:.4f}\")\n",
        "        print(f\"Relative improvement: {rel_improvement:.1f}%\")\n",
        "\n",
        "        # Create bar chart\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        models = [\"Base Model\", \"Fine-tuned Model\"]\n",
        "        accuracies = [base_acc, ft_acc]\n",
        "        bars = plt.bar(models, accuracies, color=[\"blue\", \"green\"])\n",
        "\n",
        "        # Add values on bars\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                   f'{height:.4f}', ha='center', va='bottom')\n",
        "\n",
        "        # Add improvement text\n",
        "        plt.annotate(f\"+{abs_improvement:.4f} (+{rel_improvement:.1f}%)\",\n",
        "                   xy=(1, ft_acc),\n",
        "                   xytext=(1.3, (base_acc + ft_acc)/2),\n",
        "                   arrowprops=dict(arrowstyle=\"->\", color=\"red\"))\n",
        "\n",
        "        plt.title('Model Accuracy Comparison on GSM8K Math Reasoning')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.ylim(0, max(accuracies) + 0.1)\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save figure\n",
        "        plt.savefig(output_path)\n",
        "        print(f\"Comparison visualization saved to {output_path}\")\n",
        "\n",
        "        # Show sample correct examples from fine-tuned model\n",
        "        print(\"\\n===== EXAMPLE IMPROVEMENTS =====\")\n",
        "        ft_correct = [r for r in ft_results[\"detailed_results\"] if r[\"is_correct\"]]\n",
        "        base_correct = [r for r in base_results[\"detailed_results\"] if r[\"is_correct\"]]\n",
        "\n",
        "        # Find questions where fine-tuned was correct but base was wrong\n",
        "        ft_improved = []\n",
        "        for ft_r in ft_correct:\n",
        "            q = ft_r[\"question\"]\n",
        "            # Find matching question in base results\n",
        "            base_r = next((r for r in base_results[\"detailed_results\"] if r[\"question\"] == q), None)\n",
        "            if base_r and not base_r[\"is_correct\"]:\n",
        "                ft_improved.append((base_r, ft_r))\n",
        "\n",
        "        print(f\"Found {len(ft_improved)} questions where fine-tuning improved the result\")\n",
        "\n",
        "        # Show a few examples\n",
        "        for i, (base_r, ft_r) in enumerate(ft_improved[:3]):\n",
        "            print(f\"\\nExample {i+1}:\")\n",
        "            print(f\"Question: {base_r['question'][:150]}...\" if len(base_r['question']) > 150 else f\"Question: {base_r['question']}\")\n",
        "            print(f\"Base model answer: {base_r['extracted_answer']} (❌)\")\n",
        "            print(f\"Fine-tuned model answer: {ft_r['extracted_answer']} (✅)\")\n",
        "            print(f\"True answer: {base_r['true_answer']}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating visualization: {e}\")"
      ],
      "metadata": {
        "id": "9h9LmDcm0G3N"
      },
      "id": "9h9LmDcm0G3N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Main"
      ],
      "metadata": {
        "id": "hNoH1i7i0Lxv"
      },
      "id": "hNoH1i7i0Lxv"
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Run the complete fine-tuning and evaluation pipeline\"\"\"\n",
        "    try:\n",
        "        # Model name\n",
        "        model_name = \"HuggingFaceH4/mistral-7b-sft-beta\"\n",
        "        output_dir = \"fine_tuned_mistral_math_improved\"\n",
        "\n",
        "        # 1. Evaluate base model first for proper comparison\n",
        "        print(\"\\n===== EVALUATING BASE MODEL =====\")\n",
        "        base_model, base_tokenizer = load_model_and_tokenizer(model_name)\n",
        "        base_results = evaluate_model(base_model, base_tokenizer, evaluation_set, model_name)\n",
        "\n",
        "        # Save base model results\n",
        "        with open(\"base_model_results.json\", \"w\") as f:\n",
        "            json.dump(base_results, f)\n",
        "\n",
        "        # Free memory\n",
        "        del base_model\n",
        "        clear_memory()\n",
        "\n",
        "        # 2. Process data for fine-tuning\n",
        "        print(\"\\n===== PROCESSING TRAINING DATA =====\")\n",
        "        processed_data = process_numina_data_for_training(primeintellect_data, base_tokenizer, model_name)\n",
        "\n",
        "        # 3. Fine-tune the model\n",
        "        print(\"\\n===== STARTING FINE-TUNING =====\")\n",
        "        fine_tuned_dir = fine_tune_model(\n",
        "            model_name,\n",
        "            processed_data,\n",
        "            tokenizer=base_tokenizer,\n",
        "            output_dir=output_dir\n",
        "        )\n",
        "\n",
        "        # 4. Find best checkpoint\n",
        "        best_checkpoint = find_best_checkpoint(fine_tuned_dir)\n",
        "\n",
        "        # 5. Evaluate fine-tuned model\n",
        "        print(\"\\n===== EVALUATING FINE-TUNED MODEL =====\")\n",
        "        ft_results = evaluate_finetuned_model(model_name, best_checkpoint, evaluation_set)\n",
        "\n",
        "        # 6. Create comparison visualization\n",
        "        visualize_results(base_results, ft_results, os.path.join(output_dir, \"model_comparison.png\"))\n",
        "\n",
        "        print(\"\\n===== FINE-TUNING PIPELINE COMPLETED =====\")\n",
        "        print(f\"Base model accuracy: {base_results['overall_accuracy']:.4f}\")\n",
        "        if ft_results:\n",
        "            print(f\"Fine-tuned model accuracy: {ft_results['overall_accuracy']:.4f}\")\n",
        "            improvement = ft_results['overall_accuracy'] - base_results['overall_accuracy']\n",
        "            print(f\"Absolute improvement: {improvement:.4f}\")\n",
        "            rel_improvement = (improvement / base_results['overall_accuracy'] * 100) if base_results['overall_accuracy'] > 0 else float('inf')\n",
        "            print(f\"Relative improvement: {rel_improvement:.1f}%\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main pipeline: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "id": "-ynm_tQh0IBE"
      },
      "id": "-ynm_tQh0IBE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model_on_problem(model_path, problem_text):\n",
        "    \"\"\"Test a model on a specific problem to verify it works\"\"\"\n",
        "    print(f\"\\n===== TESTING MODEL ON EXAMPLE PROBLEM =====\")\n",
        "\n",
        "    try:\n",
        "        # Determine if this is a base model or adapter\n",
        "        is_adapter = os.path.exists(os.path.join(model_path, \"adapter_config.json\"))\n",
        "\n",
        "        # Load tokenizer from either the model path or base model\n",
        "        tokenizer_path = model_path\n",
        "        if is_adapter:\n",
        "            # For adapters, get base model from adapter config\n",
        "            with open(os.path.join(model_path, \"adapter_config.json\"), \"r\") as f:\n",
        "                adapter_config = json.load(f)\n",
        "            if \"base_model_name_or_path\" in adapter_config:\n",
        "                tokenizer_path = adapter_config[\"base_model_name_or_path\"]\n",
        "            else:\n",
        "                # Default to Mistral if we can't determine base model\n",
        "                tokenizer_path = \"HuggingFaceH4/mistral-7b-sft-beta\"\n",
        "\n",
        "        # Load tokenizer\n",
        "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
        "        if not tokenizer.pad_token:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        # Format the prompt\n",
        "        prompt = format_math_prompt(problem_text)\n",
        "\n",
        "        # Format for the model\n",
        "        input_text = f\"<s>[INST] {prompt} [/INST]\"\n",
        "        input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "        # Load the model\n",
        "        if is_adapter:\n",
        "            # Load base model first\n",
        "            base_model = AutoModelForCausalLM.from_pretrained(\n",
        "                tokenizer_path,\n",
        "                device_map=\"auto\",\n",
        "                load_in_4bit=True,\n",
        "                torch_dtype=torch.float16\n",
        "            )\n",
        "            # Then load adapter\n",
        "            model = PeftModel.from_pretrained(base_model, model_path)\n",
        "            del base_model  # Free memory\n",
        "        else:\n",
        "            # Load as full model\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_path,\n",
        "                device_map=\"auto\",\n",
        "                load_in_4bit=True,\n",
        "                torch_dtype=torch.float16\n",
        "            )\n",
        "\n",
        "        # Generate response\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                input_ids.to(model.device),\n",
        "                max_new_tokens=512,\n",
        "                temperature=0.2,\n",
        "                top_p=0.95,\n",
        "                do_sample=True,\n",
        "                num_beams=2,\n",
        "                repetition_penalty=1.05,\n",
        "                pad_token_id=tokenizer.pad_token_id\n",
        "            )\n",
        "\n",
        "        # Decode and extract response\n",
        "        full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        response = full_output[len(tokenizer.decode(input_ids[0], skip_special_tokens=True)):].strip()\n",
        "\n",
        "        # Extract answer\n",
        "        extracted_answer = extract_answer(response, \"math_reasoning\")\n",
        "\n",
        "        print(f\"\\nProblem: {problem_text}\")\n",
        "        print(f\"\\nModel response:\")\n",
        "        print(response)\n",
        "        print(f\"\\nExtracted answer: {extracted_answer}\")\n",
        "\n",
        "        # Save the response\n",
        "        with open(os.path.join(os.path.dirname(model_path), \"example_response.txt\"), \"w\") as f:\n",
        "            f.write(f\"Problem: {problem_text}\\n\\n\")\n",
        "            f.write(f\"Model response:\\n{response}\\n\\n\")\n",
        "            f.write(f\"Extracted answer: {extracted_answer}\\n\")\n",
        "\n",
        "        # Free memory\n",
        "        del model\n",
        "        clear_memory()\n",
        "\n",
        "        return extracted_answer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error testing model: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# Run evaluation on a single example if full evaluation is too heavy\n",
        "def minimal_eval(model_path):\n",
        "    \"\"\"Run minimal evaluation on a single example\"\"\"\n",
        "    test_example = {\n",
        "        \"question\": \"Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\",\n",
        "        \"answer\": \"18\"\n",
        "    }\n",
        "\n",
        "    print(\"\\n===== RUNNING MINIMAL EVALUATION =====\")\n",
        "    result = test_model_on_problem(model_path, test_example[\"question\"])\n",
        "\n",
        "    if result:\n",
        "        # Clean up for comparison\n",
        "        result_clean = re.sub(r'[^\\d.-]', '', result)\n",
        "        answer_clean = re.sub(r'[^\\d.-]', '', test_example[\"answer\"])\n",
        "\n",
        "        try:\n",
        "            is_correct = float(result_clean) == float(answer_clean)\n",
        "        except:\n",
        "            is_correct = result_clean == answer_clean\n",
        "\n",
        "        print(f\"Correct answer: {test_example['answer']}\")\n",
        "        print(f\"Model answer correct: {'✓' if is_correct else '✗'}\")\n",
        "\n",
        "    print(\"\\n===== MINIMAL EVALUATION COMPLETE =====\")"
      ],
      "metadata": {
        "id": "z6Y3Es5k0Zov"
      },
      "id": "z6Y3Es5k0Zov",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        print(\"Starting improved fine-tuning pipeline...\")\n",
        "\n",
        "        main()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main execution: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "        # If full pipeline fails, try minimal evaluation on a checkpoint\n",
        "        print(\"\\nFull pipeline failed. Trying minimal evaluation...\")\n",
        "\n",
        "        # Look for any checkpoints\n",
        "        import glob\n",
        "        checkpoints = glob.glob(\"fine_tuned_mistral_math*/checkpoint-*\")\n",
        "        if checkpoints:\n",
        "            checkpoint = checkpoints[-1]  # Use the latest checkpoint\n",
        "            print(f\"Found checkpoint: {checkpoint}\")\n",
        "            minimal_eval(checkpoint)\n",
        "        else:\n",
        "            print(\"No checkpoints found for minimal evaluation.\")"
      ],
      "metadata": {
        "id": "n2oFD0Db0cqk"
      },
      "id": "n2oFD0Db0cqk",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 592.642275,
      "end_time": "2023-10-19T19:56:29.207434",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-10-19T19:46:36.565159",
      "version": "2.4.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}